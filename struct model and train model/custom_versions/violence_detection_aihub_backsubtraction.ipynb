{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"violence_detection_aihub.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-x-x2hhlOKkl","colab_type":"text"},"source":["# Violence Detection using CNN + LSTM neural netowrk"]},{"cell_type":"markdown","metadata":{"id":"9fG4ju6NOSIv","colab_type":"text"},"source":["by [Pedro.F Rodenas (Github)](https://github.com/pedrofrodenas)"]},{"cell_type":"markdown","metadata":{"id":"nRQW1dLqOVO_","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"a1iagEQ3OXEn","colab_type":"text"},"source":["Today, the amount of public violence has increased dramatically. As much in high schools as in the street. This has resulted in the ubiquitous use of surveillance cameras. This has helped the authorities to identify these events and take the necessary measures. But almost all systems today require the human-inspection of these videos to identify such events, which is virtually inefficient. It is therefore necessary to have such a practical system that can automatically monitor and identify the surveillance videos.\n","The development of various deep learning techniques, thanks to the availability of large data sets and computational resources, has resulted in a historic change in the community of computer vision. Various techniques have been developed to address problems such as object detection, recognition, tracking, action recognition, legend generation, etc. However, despite recent developments in deep learning, very few techniques based on deep learning have been proposed to address the problem of detecting violence from videos."]},{"cell_type":"markdown","metadata":{"id":"L8w5vbqMObaw","colab_type":"text"},"source":["## Flowchart"]},{"cell_type":"markdown","metadata":{"id":"6kz7yE3kOgMk","colab_type":"text"},"source":["The method consists of extracting a set of frames belonging to the video, sending them to a pretrained network called VGG16, obtaining the output of one of its final layers and from these outputs train another network architecture with a type of special neurons called LSTM. These neurons have memory and are able to analyze the temporal information of the video, if at any time they detect violence, it will be classified as a violent video.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UVXa7uMnOlkp","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"oDfDnlliPMd-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973199978,"user_tz":-540,"elapsed":8105,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["%matplotlib inline\n","import cv2\n","import os\n","import numpy as np\n","import keras\n","import matplotlib.pyplot as plt\n","from random import shuffle\n","from keras.applications import VGG16\n","from keras import backend as K\n","from keras.models import Model, Sequential\n","from keras.layers import Input\n","from keras.layers import LSTM\n","from keras.layers import Dense, Activation\n","import sys\n","import h5py"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFU6Qg8mPXvv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598962610896,"user_tz":-540,"elapsed":1171,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"3a7b9c2e-a66c-41d6-8606-a53b2294d992"},"source":["keras.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.4.3'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"dky0fSqzSumS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1598962644557,"user_tz":-540,"elapsed":32317,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"067bd016-a946-4d17-e27a-fb76003ad931"},"source":["from google.colab import drive\n","\n","drive.mount('/content/data')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v6pf1l28PzIO","colab_type":"text"},"source":["## Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"WKnyJkf8PzxE","colab_type":"text"},"source":["We will use the function ```print_progress``` to print the amount of videos processed and ```download_data``` to download the datasets"]},{"cell_type":"code","metadata":{"id":"qnafWmS7P3CG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962653525,"user_tz":-540,"elapsed":716,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["def print_progress(count, max_count):\n","    # Percentage completion.\n","    pct_complete = count / max_count\n","\n","    # Status-message. Note the \\r which means the line should\n","    # overwrite itself.\n","    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n","\n","    # Print it.\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRf-KgkjP9Kt","colab_type":"text"},"source":["## Load Data\n","\n","Firstly, we define the directory to place the video dataset"]},{"cell_type":"code","metadata":{"id":"RiRKgwBgP-NY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962656522,"user_tz":-540,"elapsed":718,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["in_dir = \"data/My Drive/new_dataset\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reLkhZglQBrD","colab_type":"text"},"source":["We set the url to download the dataset"]},{"cell_type":"markdown","metadata":{"id":"K7zrKRupRFaz","colab_type":"text"},"source":["to download the dataset and decompress it:"]},{"cell_type":"markdown","metadata":{"id":"UCZHrpJjRKky","colab_type":"text"},"source":["Copy some of the data-dimensions for convenience."]},{"cell_type":"code","metadata":{"id":"SXTNEj6SRLZZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962664010,"user_tz":-540,"elapsed":861,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["# Frame size  \n","img_size = 224\n","\n","img_size_touple = (img_size, img_size)\n","\n","# Number of channels (RGB)\n","num_channels = 3\n","\n","# Flat frame size\n","img_size_flat = img_size * img_size * num_channels\n","\n","# Number of classes for classification (Violence-No Violence)\n","num_classes = 2\n","\n","# Number of files to train\n","_num_files_train = 1\n","\n","# Number of frames per video\n","_images_per_file = 20\n","\n","# Number of frames per training set\n","_num_images_train = _num_files_train * _images_per_file\n","\n","# Video extension\n","video_exts = \".avi\"\n","\n","#for background subtraction\n","fgbg = cv2.createBackgroundSubtractorMOG2(detectShadows = False)\n","kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wodq7EaSRSS8","colab_type":"text"},"source":["### Helper-function for getting video frames\n","Function used to get 20 frames from a video file and convert the frame to a suitable format for the neural net."]},{"cell_type":"code","metadata":{"id":"eu9c4a-3RVkO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962671203,"user_tz":-540,"elapsed":956,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["#폴더와 파일명을 입력받아 동영상 파일을 이용해 20개의 image frame 생성하고 반환하는 함수\n","def get_frames(current_dir, file_name):\n","    \n","    in_file = os.path.join(current_dir, file_name)\n","    \n","    images = []\n","    \n","    vidcap = cv2.VideoCapture(in_file)\n","    \n","    success,image = vidcap.read()\n","        \n","    count = 0\n","\n","    #images_per_file == 20\n","    while count<_images_per_file:\n","        #BGR형식을 RGB로 형식으로 변경\n","        image = fgbg.apply(image)\n","        image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n","\n","        RGB_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n","        \n","        #이미지를 224 * 224 size로 변환하고 보간법 적용\n","        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n","                                 interpolation=cv2.INTER_CUBIC)\n","    \n","        images.append(res)\n","    \n","        success,image = vidcap.read()\n","    \n","        count += 1\n","        \n","    resul = np.array(images)\n","    #0~255사이의 정수 값을 신경망의 input으로 적합하게 0~1사이의 float 값으로 전처리\n","    resul = (resul / 255.).astype(np.float16)\n","    \n","    #resul의 자료형은 numpy.array, shape는 (20, 224, 224, 3)\n","    return resul"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLCjYFBtRZb-","colab_type":"text"},"source":["### Helper function to get the names of the data downloaded and label it"]},{"cell_type":"code","metadata":{"id":"Qiv5NIJjRbIA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962675654,"user_tz":-540,"elapsed":675,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["# data폴더 안의 영상파일들의 이름('fi' 또는 'no'로 시작)을 통해 폭력영상인지 비폭력 영상인지 레이블링([1,0] , [0,1])하고\n","# 랜덤하게 섞은 뒤 영상파일 이름, 레이블로 구성된 튜플 반환\n","def label_video_names(in_dir):\n","    \n","    # list containing video names\n","    names = []\n","    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n","    labels = []\n","    \n","    \n","    for current_dir, dir_names,file_names in os.walk(in_dir):\n","        \n","        for file_name in file_names:\n","            \n","            if file_name[0:2] == 'fi':\n","                labels.append([1,0])\n","                names.append(file_name)\n","            elif file_name[0:2] == 'no':\n","                labels.append([0,1])\n","                names.append(file_name)\n","                     \n","            \n","    c = list(zip(names,labels))\n","    # Suffle the data (names and labels)\n","    shuffle(c)\n","    \n","    names, labels = zip(*c)\n","            \n","    return names, labels"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3KW2kfgReKn","colab_type":"text"},"source":["### Plot a video frame to see if data is correct"]},{"cell_type":"code","metadata":{"id":"dIsaAgcyRfIx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962731879,"user_tz":-540,"elapsed":48737,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["# First get the names and labels of the whole videos\n","names, labels = label_video_names(in_dir)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFMDv12XWbNS","colab_type":"code","colab":{}},"source":["# names, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqucOMsJRgqm","colab_type":"text"},"source":["Then we are going to load 20 frames of one video, for example"]},{"cell_type":"code","metadata":{"id":"xUfZO-0BRj0f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598962780872,"user_tz":-540,"elapsed":617,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"aa6c958a-1d4e-41e9-b397-4bec3de1e699"},"source":["names[12]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'no_3051.avi'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ORGJ2pS9RnWw","colab_type":"text"},"source":["The video has violence, look at the name of the video, starts with 'fi'"]},{"cell_type":"code","metadata":{"id":"EqBi8z6rRoMW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962783811,"user_tz":-540,"elapsed":1329,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["frames = get_frames(in_dir, names[12])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"UA6CQgLoRNVl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598962785733,"user_tz":-540,"elapsed":629,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"2921aed7-313d-49f6-b7f9-7f88b7ecabcb"},"source":["frames.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 224, 224, 3)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"vtgQEmI6RrmM","colab_type":"text"},"source":["Convert back the frames to uint8 pixel format to plot the frame"]},{"cell_type":"code","metadata":{"id":"9ihSA_ogRsNU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962787792,"user_tz":-540,"elapsed":615,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["#이미지를 확인하기 위해 전처리 전으로 재변환\n","visible_frame = (frames*255).astype('uint8')"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM1kNhaHRvSv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1598962789818,"user_tz":-540,"elapsed":729,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"5c4edf3b-fbdf-4033-b293-7b4df1966b73"},"source":["plt.imshow(visible_frame[19])"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f65c3f69be0>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"_gVWtYPvR8n2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1598962793134,"user_tz":-540,"elapsed":625,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"4cc15bcb-3940-438b-a817-adad1befc24c"},"source":["plt.imshow(visible_frame[15])"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f65bfab3860>"]},"metadata":{"tags":[]},"execution_count":16},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"sF1QieG5SANp","colab_type":"text"},"source":["## Pre-Trained Model: VGG16"]},{"cell_type":"markdown","metadata":{"id":"0o6FA6slSA1l","colab_type":"text"},"source":["The following creates an instance of the pre-trained VGG16 model using the Keras API. This automatically downloads the required files if you don't have them already.\n","\n","The VGG16 model contains a convolutional part and a fully-connected (or dense) part which is used for classification. If include_top=True then the whole VGG16 model is downloaded which is about 528 MB. If include_top=False then only the convolutional part of the VGG16 model is downloaded which is just 57 MB."]},{"cell_type":"code","metadata":{"id":"MjRN6oE4SC81","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598962813407,"user_tz":-540,"elapsed":12229,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"b3bc10c5-e173-40bf-86af-1ca2e2904e14"},"source":["#분류기층과 imagenet을 통해 학습한 가중치를 포함 포함한 VGG16층 로드, \n","image_model = VGG16(include_top=True, weights='imagenet')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467904/553467096 [==============================] - 5s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P_7z-y1mSPov","colab_type":"text"},"source":["Let's see the model summary"]},{"cell_type":"code","metadata":{"id":"ud7OU0t7SQPi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":967},"executionInfo":{"status":"ok","timestamp":1598962815074,"user_tz":-540,"elapsed":706,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"79c3c9a3-c6c1-443b-f307-1ba19ee7752c"},"source":["image_model.summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","_________________________________________________________________\n","predictions (Dense)          (None, 1000)              4097000   \n","=================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-NxnGLlDSTwr","colab_type":"text"},"source":["We can observe the shape of the tensors expected as input by the pre-trained VGG16 model. In this case it is images of shape 224 x 224 x 3. Note that we have defined the frame size as 224x224x3. The video frame will be the input of the VGG16 net."]},{"cell_type":"code","metadata":{"id":"gtldUMidR4rz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598962818833,"user_tz":-540,"elapsed":581,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"b0673dea-7589-4028-cba9-c952546d3a6e"},"source":["#아래의 코드가 이렇게 수정되야 하지 않을까?\n","image_model.layers[0].output_shape[0][1:3]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(224, 224)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"9a0k89M5SUjP","colab_type":"code","colab":{}},"source":["input_shape = image_model.layers[0].output_shape[1:3]\n","input_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKYNH7V7RpON","colab_type":"code","colab":{}},"source":["#수정한 코드\n","input_shape = image_model.layers[0].output_shape[0][1:3]\n","input_shape\n","\n","#VGG16의 input size는 (batch크기, 224, 224, 3)이 될 것이다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMR6iqg4SaOl","colab_type":"text"},"source":["### VGG16 model flowchart"]},{"cell_type":"markdown","metadata":{"id":"qFUDqTiBSa0H","colab_type":"text"},"source":["The following chart shows how the data flows when using the VGG16 model for Transfer Learning. First we input and process 20 video frames in batch with the VGG16 model. Just prior to the final classification layer of the VGG16 model, we save the so-called Transfer Values to a cache-file.\n","\n","The reason for using a cache-file is that it takes a long time to process an image with the VGG16 model. If each image is processed more than once then we can save a lot of time by caching the transfer-values.\n","\n","When all the videos have been processed through the VGG16 model and the resulting transfer-values saved to a cache file, then we can use those transfer-values as the input to LSTM neural network. We will then train the second neural network using the classes from the violence dataset (Violence, No-Violence), so the network learns how to classify images based on the transfer-values from the VGG16 model."]},{"cell_type":"markdown","metadata":{"id":"5ZAgDy11SuDH","colab_type":"text"},"source":["다음의 차트는 전이학습을 위해 VGG16모델을 사용할 때 어떻게 data가 흐르는지 보여줍니다. 먼저 우리는 20개의 비디오 프레임들을 VGG16모델에 배치로 넣고 처리합니다. 그리고 VGG16 모델의 최종 분류 레이어 직전에 소위 전송 값을 캐시 파일에 저장합니다. \n","\n","캐시파일을 사용하는 이유는 VGG16모델을 이용한 처리에 많은 시간이 걸리기 때문이며, 만약 이미지가 한번 이상 처리된다면 전이 값들을 캐싱하므로써 많은 시간을 절약할 수 있습니다.\n","\n","모든 비디오가 VGG16모델을 통해 처리되고 캐시된 전이 값을 가진다면, 우리는 그 전이 값을 LSTM신경망의 input으로 사용할 수 있다. 우리는 두번 째 신경망에서 폭력 데이터 세트(폭력, 비폭력)를 사용하여 훈련시킬 것이며 \n","따라서 신경망은 VGG16모델로부터의 전이값에 기반하여 이미지를 분류할지 배운다."]},{"cell_type":"code","metadata":{"id":"4YWFA-2tSdfB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598962822386,"user_tz":-540,"elapsed":586,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"0bdc990e-a196-40f2-c6a9-6f799182db7e"},"source":["# We will use the output of the layer prior to the final\n","# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n","transfer_layer = image_model.get_layer('fc2')\n","# keras 함수형 API를 이용\n","# vgg16의 마지막 층, 즉 최종 분류 층을 짜른 모형 생성.마지막 층을 없애고 15층까지 통과한 output을 LSTM신경망에 공급할 것이다.\n","image_model_transfer = Model(inputs=image_model.input,\n","                             outputs=transfer_layer.output)\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","\n","\n","print(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n","print(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The input of the VGG16 net have dimensions: (224, 224)\n","The output of the selecter layer of VGG16 net have dimensions:  4096\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZL8B1zXHXa7q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598962825631,"user_tz":-540,"elapsed":602,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"8bd6613c-357e-4a8b-9afd-5d26ba24dee4"},"source":["#확인\n","print(K.int_shape(image_model_transfer.output)[1])\n","print(K.int_shape(transfer_layer.output)[1])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["4096\n","4096\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WfU9vd8RbPCH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":932},"executionInfo":{"status":"ok","timestamp":1598962827884,"user_tz":-540,"elapsed":586,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"a3b4b77e-7ea5-4888-c675-2caddb03e8d7"},"source":["#확인\n","image_model_transfer.summary()\n","\n","#확인결과 VGG16 -> VGG15로 변환된 것을 알 수 있다."],"execution_count":22,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","=================================================================\n","Total params: 134,260,544\n","Trainable params: 134,260,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1ghum6s2Si1n","colab_type":"text"},"source":["### Function to process 20 video frames through VGG16 and get transfer values"]},{"cell_type":"code","metadata":{"id":"-AdxYAtiSlLF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962832600,"user_tz":-540,"elapsed":602,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["#비디오의 20frames를 vgg16모델 15층(fc2)까지로 구성된 모델을 거친  output tensor를 반환하는 함수 \n","def get_transfer_values(current_dir, file_name):\n","    \n","    # Pre-allocate input-batch-array for images.\n","    # shape == (20, 224, 224, 3)\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    # image_batch의 shape == (20, 224, 224, 3)\n","    image_batch = np.zeros(shape=shape, dtype=np.float16)\n","    image_batch = get_frames(current_dir, file_name)\n","      \n","    # Pre-allocate output-array for transfer-values.\n","    # Note that we use 16-bit floating-points to save memory.\n","\n","    # shape는 (20, 4096)\n","    shape = (_images_per_file, transfer_values_size)\n","    # transfer_values의 shape == (20, 4096)\n","    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","\n","    transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","        \n","    return transfer_values"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGdRIG6oSooG","colab_type":"text"},"source":["### Generator that process one video through VGG16 each function call"]},{"cell_type":"markdown","metadata":{"id":"r3MDDReeosF9","colab_type":"text"},"source":["아래의 proces_transfer()함수를 이해하기 위해서는 파이썬 Generator공부가 필요 https://wikidocs.net/16069"]},{"cell_type":"code","metadata":{"id":"9Rj6k7Ikvfd6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1598962835285,"user_tz":-540,"elapsed":674,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"ece7826d-6b13-4403-d97c-1df581cf0a47"},"source":["#Generator 확인을 위한 dummy code cell입니다.\n","#Generator 예제\n","def test_generator():\n","\n","    count = 0\n","\n","    while(count < 3):\n","        virtual_transfer_values = (np.random.random((20,4096)) * 10).astype('int32')\n","        virtual_labelss = (np.random.random((20,2)) * 10).astype('int32')\n","        yield virtual_transfer_values, virtual_labelss\n","    \n","\n","gen = test_generator()\n","\n","print(next(gen)[0].shape)\n","print(next(gen)[1].shape)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["(20, 4096)\n","(20, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3oRbPGZvvAp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1598962838049,"user_tz":-540,"elapsed":603,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"38486ed0-5d09-4cf5-9495-78763b94aa01"},"source":["#aux의 기능 확인\n","print(np.ones([20,2]) * [1, 0])\n","print(np.ones([20,2]) * [0, 1])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[[1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","[[0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]\n"," [0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D2BvaY3eSpSH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962840997,"user_tz":-540,"elapsed":580,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["# 영상폴더 안의 영상 파일들을 모두 레이블링 값과 VGG16모델의 15층까지의 모델의 전이 값의 튜플로 반환하는 제너레이터 함수.\n","# 반복자를 생성 후 한번 호출할 때 마다 20 frame의 전이 값(20, 4096)과 레이블링 값(20, 2)를 반환\n","def proces_transfer(vid_names, in_dir, labels):\n","    \n","    count = 0\n","    #tam  = 영상 수\n","    tam = len(vid_names)\n","    \n","    # Pre-allocate input-batch-array for images.\n","    # shape(20, 224, 224, 3)\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    while count<tam:\n","        \n","        video_name = vid_names[count]\n","        # image_batch size (20, 224, 224, 3)\n","        image_batch = np.zeros(shape=shape, dtype=np.float16)\n","\n","        image_batch = get_frames(in_dir, video_name)\n","        \n","         # Note that we use 16-bit floating-points to save memory.\n","         # (20, 4096)\n","        shape = (_images_per_file, transfer_values_size)\n","        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","        \n","        # transfer_values 의 shape는 (20, 4096)\n","        transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","\n","        #labels[count] 는 이 영상이 폭력 영상인지 아닌지 [1, 0] 또는 [0, 1]의 값을 가진다.\n","        labels1 = labels[count]\n","        \n","        #이렇게 하는 이유는 폭력 영상이라면 영상을 구성하는 20 frame이 모두 폭력 frame이기 때문에 모든 frame에 labeling을 하는 것이다.\n","        aux = np.ones([20,2])\n","        \n","        labelss = labels1*aux\n","        # Generator 생성 키워드\n","        # ((20,4096), (20, 2))\n","        yield transfer_values, labelss\n","        \n","        count+=1"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-Cnv0fRSswF","colab_type":"text"},"source":["### Functions to save transfer values from VGG16 to later use\n","We are going to define functions to get the transfer values from VGG16 with defined number of files. Then save the transfer values files used from training in one file and the ones uses for testing in another one. "]},{"cell_type":"markdown","metadata":{"id":"RD9UDaVVawoL","colab_type":"text"},"source":["개조된 VGG16, 즉  VGG15를 통과한 전이 값은 \n","\n","학습이 반복되도 더 이상 학습되지 않는 값이므로 \n","\n","파일화하여 재계산 없이 빠른 접근이 가능하도록 h5파일로 만든다\n","\n","훈련용 파일과 테스트용 파일을 분리해서 생성"]},{"cell_type":"code","metadata":{"id":"tvU53ypSSvL0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973231008,"user_tz":-540,"elapsed":730,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["def make_files(n_files):\n","    \n","    gen = proces_transfer(names_training, in_dir, labels_training)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0] #20\n","    row_count2 = chunk[1].shape[0] #20\n","    \n","    with h5py.File('prueba.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:] #(None, 4096)\n","        maxshape2 = (None,) + chunk[1].shape[1:] #(None, 2)\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            dset2[row_count:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nK8uFExS0nX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962848195,"user_tz":-540,"elapsed":619,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["def make_files_test(n_files):\n","    \n","    gen = proces_transfer(names_test, in_dir, labels_test)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0]\n","    row_count2 = chunk[1].shape[0]\n","    \n","    with h5py.File('pruebavalidation.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:]\n","        maxshape2 = (None,) + chunk[1].shape[1:]\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            # 로직 오류는 발생하지 않을 수 있으나 row_count가 row_count2가 되는게 의미상 옳다.\n","            dset2[row_count2:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9axnZ8dS64T","colab_type":"text"},"source":["### Split the dataset into training set and test set\n","We are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy."]},{"cell_type":"code","metadata":{"id":"wjne3svdS9Y3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598962851358,"user_tz":-540,"elapsed":626,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["# sample수가 아주 많지 않기 때문에 셔플 시 폭력 영상과 비폭력 영상이 골고루 섞이지 않았다면\n","# 제대로 훈련이 되지 않을 수 있음 \n","# 문제해결 방법 모색 필요\n","training_set = int(len(names)*0.8)\n","test_set = int(len(names)*0.2)\n","\n","names_training = names[0:training_set]\n","names_test = names[training_set:]\n","\n","labels_training = labels[0:training_set]\n","labels_test = labels[training_set:]"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmiJi6G7TBnI","colab_type":"text"},"source":["Then we are going to process all video frames through VGG16 and save the transfer values."]},{"cell_type":"code","metadata":{"id":"pyO9WP-6TER4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598969761430,"user_tz":-540,"elapsed":6904550,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"5589b83f-5398-4c45-f932-d9c990909f90"},"source":["make_files(training_set)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["- Progress: 100.0%"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ThoefXUXPrS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598971665670,"user_tz":-540,"elapsed":1796836,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"5f312979-76a7-4ef1-bc00-dd7869ebd2ab"},"source":["make_files_test(test_set)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["- Progress: 100.0%"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0s2imcRixMAr","colab_type":"text"},"source":["### Load the cached transfer values into memory\n","We have already saved all the videos transfer values into disk. But we have to load those transfer values into memory in order to train the LSTM net. One question would be: why not process transfer values and load them into RAM memory? Yes is a more eficient way to train the second net. But if you have to train the LSTM in different ways in order to see which way gets the best accuracy, if you didn't save the transfer values into disk you would have to process the whole videos each training. It's very time consuming processing the videos through VGG16 net. \n"]},{"cell_type":"markdown","metadata":{"id":"RnK1JL-izzeS","colab_type":"text"},"source":["In order to load the saved transfer values into RAM memory we are going to use this two functions:"]},{"cell_type":"code","metadata":{"id":"Ez0blP2z0CsF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973251689,"user_tz":-540,"elapsed":727,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["def process_alldata_training():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('prueba.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dne2XaQ90MGk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973249241,"user_tz":-540,"elapsed":661,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["def process_alldata_test():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('pruebavalidation.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"phyhoYc67VW8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973374646,"user_tz":-540,"elapsed":120098,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["data, target = process_alldata_training()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"iajF2SbEjH2O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1598973119995,"user_tz":-540,"elapsed":596,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"7d2bcebd-a045-4535-8731-95ce724295fe"},"source":["#data, target 확인\n","print(len(data))\n","print(len(data[0]))\n","print(len(data[0][0]))\n","print(len(target))\n","print(len(target[0]))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["13168\n","20\n","4096\n","13168\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dJpXHvEg7Xhc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973404486,"user_tz":-540,"elapsed":147312,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["data_test, target_test = process_alldata_test()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dbarv56DkgG0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1598973124963,"user_tz":-540,"elapsed":630,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"b930021d-afff-4ee3-a240-46b9da5c6975"},"source":["#data_test, target_test 확인\n","print(len(data_test))\n","print(len(data_test[0]))\n","print(len(data_test[0][0]))\n","print(len(target_test))\n","print(len(target_test[0]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["3292\n","20\n","4096\n","3292\n","2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qjaawyqj0arq","colab_type":"text"},"source":["##Recurrent Neural Network"]},{"cell_type":"markdown","metadata":{"id":"dXFhxfzE0hGb","colab_type":"text"},"source":["The basic building block in a Recurrent Neural Network (RNN) is a Recurrent Unit (RU). There are many different variants of recurrent units such as the rather clunky LSTM (Long-Short-Term-Memory) and the somewhat simpler GRU (Gated Recurrent Unit) which we will use in this tutorial. Experiments in the literature suggest that the LSTM and GRU have roughly similar performance. Even simpler variants also exist and the literature suggests that they may perform even better than both LSTM and GRU, but they are not implemented in Keras which we will use in this tutorial.\n","\n","A recurrent neuron has an internal state that is being updated every time the unit receives a new input. This internal state serves as a kind of memory. However, it is not a traditional kind of computer memory which stores bits that are either on or off. Instead the recurrent unit stores floating-point values in its memory-state, which are read and written using matrix-operations so the operations are all differentiable. This means the memory-state can store arbitrary floating-point values (although typically limited between -1.0 and 1.0) and the network can be trained like a normal neural network using Gradient Descent.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BsIrL3ix4Edp","colab_type":"text"},"source":["### Define LSTM architecture"]},{"cell_type":"markdown","metadata":{"id":"qFiSjMaC4Q1c","colab_type":"text"},"source":["When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n"]},{"cell_type":"markdown","metadata":{"id":"HAgSUeVu58N_","colab_type":"text"},"source":["The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n"]},{"cell_type":"code","metadata":{"id":"XWABZ91b6f7l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598973410985,"user_tz":-540,"elapsed":150221,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}}},"source":["chunk_size = 4096\n","n_chunks = 20\n","rnn_size = 512\n","\n","model = Sequential()\n","model.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\n","model.add(Dense(1024))\n","model.add(Activation('relu'))\n","model.add(Dense(50))\n","model.add(Activation('sigmoid'))\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n","model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVonOPYW7F_b","colab_type":"text"},"source":["## Model training\n"]},{"cell_type":"code","metadata":{"id":"iRZlW4ZV_ygS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1598974773040,"user_tz":-540,"elapsed":1510202,"user":{"displayName":"박지민","photoUrl":"","userId":"01128535303668739690"}},"outputId":"fb42a220-074d-4f8e-f762-b50dc903c493"},"source":["epoch = 500\n","batchS = 50\n","\n","history = model.fit(np.array(data[0:10000]), np.array(target[0:10000]), epochs=epoch,\n","                    validation_data=(np.array(data[10000:]), np.array(target[10000:])), \n","                    batch_size=batchS, verbose=2)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/500\n","200/200 - 5s - loss: 0.2467 - accuracy: 0.5656 - val_loss: 0.2404 - val_accuracy: 0.5925\n","Epoch 2/500\n","200/200 - 5s - loss: 0.2306 - accuracy: 0.5854 - val_loss: 0.2297 - val_accuracy: 0.6057\n","Epoch 3/500\n","200/200 - 5s - loss: 0.2293 - accuracy: 0.5926 - val_loss: 0.2274 - val_accuracy: 0.5811\n","Epoch 4/500\n","200/200 - 5s - loss: 0.2222 - accuracy: 0.6292 - val_loss: 0.2200 - val_accuracy: 0.6572\n","Epoch 5/500\n","200/200 - 5s - loss: 0.2220 - accuracy: 0.6363 - val_loss: 0.2151 - val_accuracy: 0.6670\n","Epoch 6/500\n","200/200 - 5s - loss: 0.2169 - accuracy: 0.6550 - val_loss: 0.2171 - val_accuracy: 0.6660\n","Epoch 7/500\n","200/200 - 5s - loss: 0.2172 - accuracy: 0.6507 - val_loss: 0.2130 - val_accuracy: 0.6727\n","Epoch 8/500\n","200/200 - 5s - loss: 0.2170 - accuracy: 0.6516 - val_loss: 0.2156 - val_accuracy: 0.6629\n","Epoch 9/500\n","200/200 - 5s - loss: 0.2174 - accuracy: 0.6552 - val_loss: 0.2176 - val_accuracy: 0.6588\n","Epoch 10/500\n","200/200 - 5s - loss: 0.2143 - accuracy: 0.6584 - val_loss: 0.2191 - val_accuracy: 0.6534\n","Epoch 11/500\n","200/200 - 5s - loss: 0.2132 - accuracy: 0.6661 - val_loss: 0.2114 - val_accuracy: 0.6733\n","Epoch 12/500\n","200/200 - 5s - loss: 0.2141 - accuracy: 0.6591 - val_loss: 0.2156 - val_accuracy: 0.6692\n","Epoch 13/500\n","200/200 - 5s - loss: 0.2124 - accuracy: 0.6633 - val_loss: 0.2158 - val_accuracy: 0.6556\n","Epoch 14/500\n","200/200 - 5s - loss: 0.2136 - accuracy: 0.6569 - val_loss: 0.2245 - val_accuracy: 0.5458\n","Epoch 15/500\n","200/200 - 5s - loss: 0.2126 - accuracy: 0.6633 - val_loss: 0.2143 - val_accuracy: 0.6682\n","Epoch 16/500\n","200/200 - 5s - loss: 0.2142 - accuracy: 0.6570 - val_loss: 0.2171 - val_accuracy: 0.6569\n","Epoch 17/500\n","200/200 - 5s - loss: 0.2120 - accuracy: 0.6590 - val_loss: 0.2157 - val_accuracy: 0.6761\n","Epoch 18/500\n","200/200 - 5s - loss: 0.2123 - accuracy: 0.6605 - val_loss: 0.2174 - val_accuracy: 0.6528\n","Epoch 19/500\n","200/200 - 5s - loss: 0.2127 - accuracy: 0.6609 - val_loss: 0.2150 - val_accuracy: 0.6689\n","Epoch 20/500\n","200/200 - 5s - loss: 0.2107 - accuracy: 0.6653 - val_loss: 0.2179 - val_accuracy: 0.6717\n","Epoch 21/500\n","200/200 - 5s - loss: 0.2114 - accuracy: 0.6607 - val_loss: 0.2218 - val_accuracy: 0.6443\n","Epoch 22/500\n","200/200 - 5s - loss: 0.2109 - accuracy: 0.6647 - val_loss: 0.2119 - val_accuracy: 0.6667\n","Epoch 23/500\n","200/200 - 5s - loss: 0.2112 - accuracy: 0.6665 - val_loss: 0.2259 - val_accuracy: 0.6351\n","Epoch 24/500\n","200/200 - 5s - loss: 0.2120 - accuracy: 0.6672 - val_loss: 0.2153 - val_accuracy: 0.6686\n","Epoch 25/500\n","200/200 - 5s - loss: 0.2107 - accuracy: 0.6630 - val_loss: 0.2102 - val_accuracy: 0.6752\n","Epoch 26/500\n","200/200 - 5s - loss: 0.2126 - accuracy: 0.6633 - val_loss: 0.2100 - val_accuracy: 0.6723\n","Epoch 27/500\n","200/200 - 5s - loss: 0.2101 - accuracy: 0.6668 - val_loss: 0.2096 - val_accuracy: 0.6780\n","Epoch 28/500\n","200/200 - 5s - loss: 0.2124 - accuracy: 0.6598 - val_loss: 0.2171 - val_accuracy: 0.6664\n","Epoch 29/500\n","200/200 - 5s - loss: 0.2104 - accuracy: 0.6669 - val_loss: 0.2095 - val_accuracy: 0.6755\n","Epoch 30/500\n","200/200 - 5s - loss: 0.2110 - accuracy: 0.6689 - val_loss: 0.2121 - val_accuracy: 0.6777\n","Epoch 31/500\n","200/200 - 5s - loss: 0.2095 - accuracy: 0.6738 - val_loss: 0.2090 - val_accuracy: 0.6783\n","Epoch 32/500\n","200/200 - 5s - loss: 0.2103 - accuracy: 0.6596 - val_loss: 0.2090 - val_accuracy: 0.6761\n","Epoch 33/500\n","200/200 - 5s - loss: 0.2093 - accuracy: 0.6646 - val_loss: 0.2199 - val_accuracy: 0.5521\n","Epoch 34/500\n","200/200 - 5s - loss: 0.2097 - accuracy: 0.6664 - val_loss: 0.2091 - val_accuracy: 0.6768\n","Epoch 35/500\n","200/200 - 5s - loss: 0.2091 - accuracy: 0.6704 - val_loss: 0.2134 - val_accuracy: 0.6708\n","Epoch 36/500\n","200/200 - 5s - loss: 0.2090 - accuracy: 0.6620 - val_loss: 0.2133 - val_accuracy: 0.6711\n","Epoch 37/500\n","200/200 - 5s - loss: 0.2087 - accuracy: 0.6722 - val_loss: 0.2091 - val_accuracy: 0.6739\n","Epoch 38/500\n","200/200 - 5s - loss: 0.2103 - accuracy: 0.6662 - val_loss: 0.2156 - val_accuracy: 0.6698\n","Epoch 39/500\n","200/200 - 5s - loss: 0.2091 - accuracy: 0.6699 - val_loss: 0.2106 - val_accuracy: 0.6742\n","Epoch 40/500\n","200/200 - 5s - loss: 0.2090 - accuracy: 0.6693 - val_loss: 0.2089 - val_accuracy: 0.6758\n","Epoch 41/500\n","200/200 - 5s - loss: 0.2081 - accuracy: 0.6736 - val_loss: 0.2164 - val_accuracy: 0.6708\n","Epoch 42/500\n","200/200 - 5s - loss: 0.2098 - accuracy: 0.6707 - val_loss: 0.2100 - val_accuracy: 0.6730\n","Epoch 43/500\n","200/200 - 5s - loss: 0.2083 - accuracy: 0.6701 - val_loss: 0.2098 - val_accuracy: 0.6758\n","Epoch 44/500\n","200/200 - 5s - loss: 0.2085 - accuracy: 0.6715 - val_loss: 0.2108 - val_accuracy: 0.6698\n","Epoch 45/500\n","200/200 - 5s - loss: 0.2091 - accuracy: 0.6735 - val_loss: 0.2222 - val_accuracy: 0.6537\n","Epoch 46/500\n","200/200 - 5s - loss: 0.2083 - accuracy: 0.6736 - val_loss: 0.2213 - val_accuracy: 0.6531\n","Epoch 47/500\n","200/200 - 5s - loss: 0.2078 - accuracy: 0.6749 - val_loss: 0.2115 - val_accuracy: 0.6711\n","Epoch 48/500\n","200/200 - 5s - loss: 0.2081 - accuracy: 0.6694 - val_loss: 0.2088 - val_accuracy: 0.6821\n","Epoch 49/500\n","200/200 - 5s - loss: 0.2081 - accuracy: 0.6732 - val_loss: 0.2093 - val_accuracy: 0.6812\n","Epoch 50/500\n","200/200 - 5s - loss: 0.2074 - accuracy: 0.6737 - val_loss: 0.2134 - val_accuracy: 0.6679\n","Epoch 51/500\n","200/200 - 5s - loss: 0.2070 - accuracy: 0.6707 - val_loss: 0.2077 - val_accuracy: 0.6793\n","Epoch 52/500\n","200/200 - 5s - loss: 0.2070 - accuracy: 0.6731 - val_loss: 0.2076 - val_accuracy: 0.6758\n","Epoch 53/500\n","200/200 - 5s - loss: 0.2075 - accuracy: 0.6695 - val_loss: 0.2091 - val_accuracy: 0.6777\n","Epoch 54/500\n","200/200 - 5s - loss: 0.2053 - accuracy: 0.6765 - val_loss: 0.2082 - val_accuracy: 0.6727\n","Epoch 55/500\n","200/200 - 5s - loss: 0.2065 - accuracy: 0.6755 - val_loss: 0.2068 - val_accuracy: 0.6790\n","Epoch 56/500\n","200/200 - 5s - loss: 0.2054 - accuracy: 0.6779 - val_loss: 0.2100 - val_accuracy: 0.6809\n","Epoch 57/500\n","200/200 - 5s - loss: 0.2065 - accuracy: 0.6784 - val_loss: 0.2075 - val_accuracy: 0.6749\n","Epoch 58/500\n","200/200 - 5s - loss: 0.2056 - accuracy: 0.6778 - val_loss: 0.2107 - val_accuracy: 0.6714\n","Epoch 59/500\n","200/200 - 5s - loss: 0.2059 - accuracy: 0.6748 - val_loss: 0.2069 - val_accuracy: 0.6774\n","Epoch 60/500\n","200/200 - 5s - loss: 0.2053 - accuracy: 0.6748 - val_loss: 0.2071 - val_accuracy: 0.6824\n","Epoch 61/500\n","200/200 - 5s - loss: 0.2058 - accuracy: 0.6764 - val_loss: 0.2071 - val_accuracy: 0.6771\n","Epoch 62/500\n","200/200 - 5s - loss: 0.2053 - accuracy: 0.6793 - val_loss: 0.2086 - val_accuracy: 0.6783\n","Epoch 63/500\n","200/200 - 5s - loss: 0.2060 - accuracy: 0.6752 - val_loss: 0.2097 - val_accuracy: 0.6771\n","Epoch 64/500\n","200/200 - 5s - loss: 0.2076 - accuracy: 0.6731 - val_loss: 0.2079 - val_accuracy: 0.6818\n","Epoch 65/500\n","200/200 - 5s - loss: 0.2043 - accuracy: 0.6778 - val_loss: 0.2178 - val_accuracy: 0.6600\n","Epoch 66/500\n","200/200 - 5s - loss: 0.2058 - accuracy: 0.6736 - val_loss: 0.2083 - val_accuracy: 0.6774\n","Epoch 67/500\n","200/200 - 5s - loss: 0.2063 - accuracy: 0.6760 - val_loss: 0.2141 - val_accuracy: 0.6806\n","Epoch 68/500\n","200/200 - 5s - loss: 0.2066 - accuracy: 0.6729 - val_loss: 0.2062 - val_accuracy: 0.6783\n","Epoch 69/500\n","200/200 - 5s - loss: 0.2053 - accuracy: 0.6769 - val_loss: 0.2070 - val_accuracy: 0.6824\n","Epoch 70/500\n","200/200 - 5s - loss: 0.2045 - accuracy: 0.6795 - val_loss: 0.2118 - val_accuracy: 0.6793\n","Epoch 71/500\n","200/200 - 5s - loss: 0.2038 - accuracy: 0.6810 - val_loss: 0.2057 - val_accuracy: 0.6821\n","Epoch 72/500\n","200/200 - 5s - loss: 0.2044 - accuracy: 0.6774 - val_loss: 0.2118 - val_accuracy: 0.6755\n","Epoch 73/500\n","200/200 - 5s - loss: 0.2040 - accuracy: 0.6768 - val_loss: 0.2097 - val_accuracy: 0.6717\n","Epoch 74/500\n","200/200 - 5s - loss: 0.2054 - accuracy: 0.6752 - val_loss: 0.2090 - val_accuracy: 0.6796\n","Epoch 75/500\n","200/200 - 5s - loss: 0.2036 - accuracy: 0.6821 - val_loss: 0.2071 - val_accuracy: 0.6793\n","Epoch 76/500\n","200/200 - 5s - loss: 0.2053 - accuracy: 0.6771 - val_loss: 0.2062 - val_accuracy: 0.6818\n","Epoch 77/500\n","200/200 - 5s - loss: 0.2049 - accuracy: 0.6750 - val_loss: 0.2072 - val_accuracy: 0.6831\n","Epoch 78/500\n","200/200 - 5s - loss: 0.2039 - accuracy: 0.6768 - val_loss: 0.2058 - val_accuracy: 0.6777\n","Epoch 79/500\n","200/200 - 5s - loss: 0.2043 - accuracy: 0.6767 - val_loss: 0.2068 - val_accuracy: 0.6853\n","Epoch 80/500\n","200/200 - 5s - loss: 0.2043 - accuracy: 0.6768 - val_loss: 0.2106 - val_accuracy: 0.6802\n","Epoch 81/500\n","200/200 - 5s - loss: 0.2029 - accuracy: 0.6818 - val_loss: 0.2058 - val_accuracy: 0.6821\n","Epoch 82/500\n","200/200 - 5s - loss: 0.2031 - accuracy: 0.6806 - val_loss: 0.2118 - val_accuracy: 0.6771\n","Epoch 83/500\n","200/200 - 5s - loss: 0.2025 - accuracy: 0.6809 - val_loss: 0.2072 - val_accuracy: 0.6831\n","Epoch 84/500\n","200/200 - 5s - loss: 0.2022 - accuracy: 0.6844 - val_loss: 0.2071 - val_accuracy: 0.6796\n","Epoch 85/500\n","200/200 - 5s - loss: 0.2032 - accuracy: 0.6771 - val_loss: 0.2078 - val_accuracy: 0.6815\n","Epoch 86/500\n","200/200 - 5s - loss: 0.2030 - accuracy: 0.6788 - val_loss: 0.2185 - val_accuracy: 0.6632\n","Epoch 87/500\n","200/200 - 5s - loss: 0.2034 - accuracy: 0.6809 - val_loss: 0.2076 - val_accuracy: 0.6815\n","Epoch 88/500\n","200/200 - 5s - loss: 0.2026 - accuracy: 0.6817 - val_loss: 0.2055 - val_accuracy: 0.6806\n","Epoch 89/500\n","200/200 - 5s - loss: 0.2030 - accuracy: 0.6787 - val_loss: 0.2061 - val_accuracy: 0.6824\n","Epoch 90/500\n","200/200 - 5s - loss: 0.2039 - accuracy: 0.6769 - val_loss: 0.2132 - val_accuracy: 0.6604\n","Epoch 91/500\n","200/200 - 5s - loss: 0.2035 - accuracy: 0.6796 - val_loss: 0.2059 - val_accuracy: 0.6828\n","Epoch 92/500\n","200/200 - 5s - loss: 0.2021 - accuracy: 0.6852 - val_loss: 0.2122 - val_accuracy: 0.6739\n","Epoch 93/500\n","200/200 - 5s - loss: 0.2052 - accuracy: 0.6735 - val_loss: 0.2094 - val_accuracy: 0.6768\n","Epoch 94/500\n","200/200 - 5s - loss: 0.2017 - accuracy: 0.6848 - val_loss: 0.2051 - val_accuracy: 0.6809\n","Epoch 95/500\n","200/200 - 5s - loss: 0.2020 - accuracy: 0.6822 - val_loss: 0.2056 - val_accuracy: 0.6850\n","Epoch 96/500\n","200/200 - 5s - loss: 0.2023 - accuracy: 0.6805 - val_loss: 0.2069 - val_accuracy: 0.6802\n","Epoch 97/500\n","200/200 - 5s - loss: 0.2027 - accuracy: 0.6790 - val_loss: 0.2049 - val_accuracy: 0.6799\n","Epoch 98/500\n","200/200 - 5s - loss: 0.2017 - accuracy: 0.6832 - val_loss: 0.2043 - val_accuracy: 0.6790\n","Epoch 99/500\n","200/200 - 5s - loss: 0.2021 - accuracy: 0.6754 - val_loss: 0.2045 - val_accuracy: 0.6812\n","Epoch 100/500\n","200/200 - 5s - loss: 0.2016 - accuracy: 0.6854 - val_loss: 0.2066 - val_accuracy: 0.6853\n","Epoch 101/500\n","200/200 - 5s - loss: 0.2035 - accuracy: 0.6781 - val_loss: 0.2083 - val_accuracy: 0.6723\n","Epoch 102/500\n","200/200 - 5s - loss: 0.2007 - accuracy: 0.6844 - val_loss: 0.2082 - val_accuracy: 0.6853\n","Epoch 103/500\n","200/200 - 5s - loss: 0.1997 - accuracy: 0.6875 - val_loss: 0.2099 - val_accuracy: 0.6730\n","Epoch 104/500\n","200/200 - 5s - loss: 0.1993 - accuracy: 0.6860 - val_loss: 0.2050 - val_accuracy: 0.6818\n","Epoch 105/500\n","200/200 - 5s - loss: 0.2015 - accuracy: 0.6837 - val_loss: 0.2085 - val_accuracy: 0.6802\n","Epoch 106/500\n","200/200 - 5s - loss: 0.2000 - accuracy: 0.6848 - val_loss: 0.2042 - val_accuracy: 0.6853\n","Epoch 107/500\n","200/200 - 5s - loss: 0.2010 - accuracy: 0.6839 - val_loss: 0.2037 - val_accuracy: 0.6831\n","Epoch 108/500\n","200/200 - 5s - loss: 0.2006 - accuracy: 0.6828 - val_loss: 0.2107 - val_accuracy: 0.6705\n","Epoch 109/500\n","200/200 - 5s - loss: 0.2016 - accuracy: 0.6849 - val_loss: 0.2042 - val_accuracy: 0.6859\n","Epoch 110/500\n","200/200 - 5s - loss: 0.1994 - accuracy: 0.6892 - val_loss: 0.2100 - val_accuracy: 0.6847\n","Epoch 111/500\n","200/200 - 5s - loss: 0.2035 - accuracy: 0.6751 - val_loss: 0.2087 - val_accuracy: 0.6856\n","Epoch 112/500\n","200/200 - 5s - loss: 0.2006 - accuracy: 0.6840 - val_loss: 0.2076 - val_accuracy: 0.6736\n","Epoch 113/500\n","200/200 - 5s - loss: 0.2001 - accuracy: 0.6864 - val_loss: 0.2026 - val_accuracy: 0.6853\n","Epoch 114/500\n","200/200 - 5s - loss: 0.1997 - accuracy: 0.6880 - val_loss: 0.2116 - val_accuracy: 0.6664\n","Epoch 115/500\n","200/200 - 5s - loss: 0.2001 - accuracy: 0.6858 - val_loss: 0.2070 - val_accuracy: 0.6752\n","Epoch 116/500\n","200/200 - 5s - loss: 0.1996 - accuracy: 0.6856 - val_loss: 0.2064 - val_accuracy: 0.6834\n","Epoch 117/500\n","200/200 - 5s - loss: 0.1992 - accuracy: 0.6883 - val_loss: 0.2042 - val_accuracy: 0.6900\n","Epoch 118/500\n","200/200 - 5s - loss: 0.1979 - accuracy: 0.6885 - val_loss: 0.2107 - val_accuracy: 0.6824\n","Epoch 119/500\n","200/200 - 5s - loss: 0.1994 - accuracy: 0.6857 - val_loss: 0.2054 - val_accuracy: 0.6869\n","Epoch 120/500\n","200/200 - 5s - loss: 0.1998 - accuracy: 0.6831 - val_loss: 0.2020 - val_accuracy: 0.6853\n","Epoch 121/500\n","200/200 - 5s - loss: 0.2002 - accuracy: 0.6836 - val_loss: 0.2069 - val_accuracy: 0.6793\n","Epoch 122/500\n","200/200 - 5s - loss: 0.1986 - accuracy: 0.6828 - val_loss: 0.2029 - val_accuracy: 0.6856\n","Epoch 123/500\n","200/200 - 5s - loss: 0.1995 - accuracy: 0.6833 - val_loss: 0.2056 - val_accuracy: 0.6736\n","Epoch 124/500\n","200/200 - 5s - loss: 0.1981 - accuracy: 0.6891 - val_loss: 0.2117 - val_accuracy: 0.6667\n","Epoch 125/500\n","200/200 - 5s - loss: 0.1978 - accuracy: 0.6905 - val_loss: 0.2046 - val_accuracy: 0.6796\n","Epoch 126/500\n","200/200 - 5s - loss: 0.1971 - accuracy: 0.6926 - val_loss: 0.2035 - val_accuracy: 0.6875\n","Epoch 127/500\n","200/200 - 5s - loss: 0.1996 - accuracy: 0.6890 - val_loss: 0.2020 - val_accuracy: 0.6862\n","Epoch 128/500\n","200/200 - 5s - loss: 0.1967 - accuracy: 0.6886 - val_loss: 0.2077 - val_accuracy: 0.6730\n","Epoch 129/500\n","200/200 - 5s - loss: 0.1988 - accuracy: 0.6896 - val_loss: 0.2046 - val_accuracy: 0.6739\n","Epoch 130/500\n","200/200 - 5s - loss: 0.1995 - accuracy: 0.6859 - val_loss: 0.2041 - val_accuracy: 0.6837\n","Epoch 131/500\n","200/200 - 5s - loss: 0.1991 - accuracy: 0.6872 - val_loss: 0.2019 - val_accuracy: 0.6866\n","Epoch 132/500\n","200/200 - 5s - loss: 0.1964 - accuracy: 0.6934 - val_loss: 0.2074 - val_accuracy: 0.6338\n","Epoch 133/500\n","200/200 - 5s - loss: 0.1968 - accuracy: 0.6866 - val_loss: 0.2048 - val_accuracy: 0.6531\n","Epoch 134/500\n","200/200 - 5s - loss: 0.1960 - accuracy: 0.6888 - val_loss: 0.2006 - val_accuracy: 0.6872\n","Epoch 135/500\n","200/200 - 5s - loss: 0.2010 - accuracy: 0.6727 - val_loss: 0.2017 - val_accuracy: 0.6806\n","Epoch 136/500\n","200/200 - 5s - loss: 0.1955 - accuracy: 0.6934 - val_loss: 0.2025 - val_accuracy: 0.6878\n","Epoch 137/500\n","200/200 - 5s - loss: 0.1976 - accuracy: 0.6891 - val_loss: 0.2069 - val_accuracy: 0.6884\n","Epoch 138/500\n","200/200 - 5s - loss: 0.1974 - accuracy: 0.6866 - val_loss: 0.2060 - val_accuracy: 0.6884\n","Epoch 139/500\n","200/200 - 5s - loss: 0.1990 - accuracy: 0.6854 - val_loss: 0.2121 - val_accuracy: 0.6651\n","Epoch 140/500\n","200/200 - 5s - loss: 0.1966 - accuracy: 0.6886 - val_loss: 0.2020 - val_accuracy: 0.6935\n","Epoch 141/500\n","200/200 - 5s - loss: 0.1971 - accuracy: 0.6900 - val_loss: 0.2040 - val_accuracy: 0.6884\n","Epoch 142/500\n","200/200 - 5s - loss: 0.1954 - accuracy: 0.6914 - val_loss: 0.2108 - val_accuracy: 0.6708\n","Epoch 143/500\n","200/200 - 5s - loss: 0.1951 - accuracy: 0.6937 - val_loss: 0.2007 - val_accuracy: 0.6926\n","Epoch 144/500\n","200/200 - 5s - loss: 0.1949 - accuracy: 0.6960 - val_loss: 0.2050 - val_accuracy: 0.6812\n","Epoch 145/500\n","200/200 - 5s - loss: 0.1956 - accuracy: 0.6855 - val_loss: 0.2039 - val_accuracy: 0.6824\n","Epoch 146/500\n","200/200 - 5s - loss: 0.1972 - accuracy: 0.6891 - val_loss: 0.2161 - val_accuracy: 0.6619\n","Epoch 147/500\n","200/200 - 5s - loss: 0.1956 - accuracy: 0.6933 - val_loss: 0.2009 - val_accuracy: 0.6935\n","Epoch 148/500\n","200/200 - 5s - loss: 0.1964 - accuracy: 0.6885 - val_loss: 0.2064 - val_accuracy: 0.6793\n","Epoch 149/500\n","200/200 - 5s - loss: 0.1959 - accuracy: 0.6902 - val_loss: 0.2026 - val_accuracy: 0.6907\n","Epoch 150/500\n","200/200 - 5s - loss: 0.1963 - accuracy: 0.6877 - val_loss: 0.2019 - val_accuracy: 0.6809\n","Epoch 151/500\n","200/200 - 5s - loss: 0.1932 - accuracy: 0.6966 - val_loss: 0.2049 - val_accuracy: 0.6746\n","Epoch 152/500\n","200/200 - 5s - loss: 0.1951 - accuracy: 0.6939 - val_loss: 0.2073 - val_accuracy: 0.6727\n","Epoch 153/500\n","200/200 - 5s - loss: 0.1971 - accuracy: 0.6855 - val_loss: 0.2158 - val_accuracy: 0.5979\n","Epoch 154/500\n","200/200 - 5s - loss: 0.1953 - accuracy: 0.6903 - val_loss: 0.2046 - val_accuracy: 0.6919\n","Epoch 155/500\n","200/200 - 5s - loss: 0.1945 - accuracy: 0.6958 - val_loss: 0.2016 - val_accuracy: 0.6894\n","Epoch 156/500\n","200/200 - 5s - loss: 0.1965 - accuracy: 0.6896 - val_loss: 0.2027 - val_accuracy: 0.6828\n","Epoch 157/500\n","200/200 - 5s - loss: 0.1942 - accuracy: 0.6967 - val_loss: 0.2102 - val_accuracy: 0.6051\n","Epoch 158/500\n","200/200 - 5s - loss: 0.1959 - accuracy: 0.6874 - val_loss: 0.1994 - val_accuracy: 0.6875\n","Epoch 159/500\n","200/200 - 5s - loss: 0.1951 - accuracy: 0.6916 - val_loss: 0.2021 - val_accuracy: 0.6787\n","Epoch 160/500\n","200/200 - 5s - loss: 0.1959 - accuracy: 0.6909 - val_loss: 0.2113 - val_accuracy: 0.6701\n","Epoch 161/500\n","200/200 - 5s - loss: 0.1933 - accuracy: 0.6933 - val_loss: 0.2083 - val_accuracy: 0.6834\n","Epoch 162/500\n","200/200 - 5s - loss: 0.1952 - accuracy: 0.6893 - val_loss: 0.1992 - val_accuracy: 0.6935\n","Epoch 163/500\n","200/200 - 5s - loss: 0.1945 - accuracy: 0.6928 - val_loss: 0.2048 - val_accuracy: 0.6799\n","Epoch 164/500\n","200/200 - 5s - loss: 0.1947 - accuracy: 0.6944 - val_loss: 0.2051 - val_accuracy: 0.6749\n","Epoch 165/500\n","200/200 - 5s - loss: 0.1951 - accuracy: 0.6897 - val_loss: 0.2078 - val_accuracy: 0.6843\n","Epoch 166/500\n","200/200 - 5s - loss: 0.1949 - accuracy: 0.6930 - val_loss: 0.2022 - val_accuracy: 0.6834\n","Epoch 167/500\n","200/200 - 5s - loss: 0.1924 - accuracy: 0.6941 - val_loss: 0.2004 - val_accuracy: 0.6847\n","Epoch 168/500\n","200/200 - 5s - loss: 0.1945 - accuracy: 0.6939 - val_loss: 0.2009 - val_accuracy: 0.6872\n","Epoch 169/500\n","200/200 - 5s - loss: 0.1934 - accuracy: 0.6936 - val_loss: 0.2018 - val_accuracy: 0.6695\n","Epoch 170/500\n","200/200 - 5s - loss: 0.1916 - accuracy: 0.6987 - val_loss: 0.2008 - val_accuracy: 0.6884\n","Epoch 171/500\n","200/200 - 5s - loss: 0.1957 - accuracy: 0.6876 - val_loss: 0.2056 - val_accuracy: 0.6733\n","Epoch 172/500\n","200/200 - 5s - loss: 0.1941 - accuracy: 0.6905 - val_loss: 0.2145 - val_accuracy: 0.6032\n","Epoch 173/500\n","200/200 - 5s - loss: 0.1941 - accuracy: 0.6901 - val_loss: 0.2031 - val_accuracy: 0.6758\n","Epoch 174/500\n","200/200 - 5s - loss: 0.1943 - accuracy: 0.6948 - val_loss: 0.2072 - val_accuracy: 0.6717\n","Epoch 175/500\n","200/200 - 5s - loss: 0.1955 - accuracy: 0.6880 - val_loss: 0.1986 - val_accuracy: 0.6954\n","Epoch 176/500\n","200/200 - 5s - loss: 0.1940 - accuracy: 0.6893 - val_loss: 0.1988 - val_accuracy: 0.6926\n","Epoch 177/500\n","200/200 - 5s - loss: 0.1925 - accuracy: 0.6940 - val_loss: 0.1991 - val_accuracy: 0.6869\n","Epoch 178/500\n","200/200 - 5s - loss: 0.1901 - accuracy: 0.6982 - val_loss: 0.1996 - val_accuracy: 0.6796\n","Epoch 179/500\n","200/200 - 5s - loss: 0.1936 - accuracy: 0.6924 - val_loss: 0.2012 - val_accuracy: 0.6818\n","Epoch 180/500\n","200/200 - 5s - loss: 0.1924 - accuracy: 0.6916 - val_loss: 0.1982 - val_accuracy: 0.6935\n","Epoch 181/500\n","200/200 - 5s - loss: 0.1916 - accuracy: 0.6967 - val_loss: 0.2016 - val_accuracy: 0.6840\n","Epoch 182/500\n","200/200 - 5s - loss: 0.1910 - accuracy: 0.6976 - val_loss: 0.2024 - val_accuracy: 0.6787\n","Epoch 183/500\n","200/200 - 5s - loss: 0.1940 - accuracy: 0.6869 - val_loss: 0.1999 - val_accuracy: 0.6897\n","Epoch 184/500\n","200/200 - 5s - loss: 0.1943 - accuracy: 0.6918 - val_loss: 0.2146 - val_accuracy: 0.6657\n","Epoch 185/500\n","200/200 - 5s - loss: 0.1924 - accuracy: 0.6935 - val_loss: 0.2011 - val_accuracy: 0.6922\n","Epoch 186/500\n","200/200 - 5s - loss: 0.1939 - accuracy: 0.6918 - val_loss: 0.2421 - val_accuracy: 0.5928\n","Epoch 187/500\n","200/200 - 5s - loss: 0.1941 - accuracy: 0.6901 - val_loss: 0.1996 - val_accuracy: 0.6878\n","Epoch 188/500\n","200/200 - 5s - loss: 0.1915 - accuracy: 0.6937 - val_loss: 0.2096 - val_accuracy: 0.6616\n","Epoch 189/500\n","200/200 - 5s - loss: 0.1900 - accuracy: 0.6958 - val_loss: 0.1978 - val_accuracy: 0.6935\n","Epoch 190/500\n","200/200 - 5s - loss: 0.1919 - accuracy: 0.6954 - val_loss: 0.2010 - val_accuracy: 0.6768\n","Epoch 191/500\n","200/200 - 5s - loss: 0.1905 - accuracy: 0.6916 - val_loss: 0.1979 - val_accuracy: 0.6859\n","Epoch 192/500\n","200/200 - 5s - loss: 0.1910 - accuracy: 0.6891 - val_loss: 0.1991 - val_accuracy: 0.6790\n","Epoch 193/500\n","200/200 - 5s - loss: 0.1905 - accuracy: 0.6981 - val_loss: 0.2060 - val_accuracy: 0.6379\n","Epoch 194/500\n","200/200 - 5s - loss: 0.1926 - accuracy: 0.6914 - val_loss: 0.2067 - val_accuracy: 0.6758\n","Epoch 195/500\n","200/200 - 5s - loss: 0.1915 - accuracy: 0.6963 - val_loss: 0.2006 - val_accuracy: 0.6875\n","Epoch 196/500\n","200/200 - 5s - loss: 0.1887 - accuracy: 0.7048 - val_loss: 0.2063 - val_accuracy: 0.6525\n","Epoch 197/500\n","200/200 - 5s - loss: 0.1930 - accuracy: 0.6914 - val_loss: 0.2048 - val_accuracy: 0.6866\n","Epoch 198/500\n","200/200 - 5s - loss: 0.1892 - accuracy: 0.7021 - val_loss: 0.1997 - val_accuracy: 0.6866\n","Epoch 199/500\n","200/200 - 5s - loss: 0.1893 - accuracy: 0.7071 - val_loss: 0.1961 - val_accuracy: 0.6960\n","Epoch 200/500\n","200/200 - 5s - loss: 0.1886 - accuracy: 0.6991 - val_loss: 0.1986 - val_accuracy: 0.6862\n","Epoch 201/500\n","200/200 - 5s - loss: 0.1861 - accuracy: 0.7090 - val_loss: 0.1982 - val_accuracy: 0.6897\n","Epoch 202/500\n","200/200 - 5s - loss: 0.1901 - accuracy: 0.6957 - val_loss: 0.1989 - val_accuracy: 0.6742\n","Epoch 203/500\n","200/200 - 5s - loss: 0.1874 - accuracy: 0.7059 - val_loss: 0.1962 - val_accuracy: 0.6907\n","Epoch 204/500\n","200/200 - 5s - loss: 0.1886 - accuracy: 0.7025 - val_loss: 0.2066 - val_accuracy: 0.6793\n","Epoch 205/500\n","200/200 - 5s - loss: 0.1869 - accuracy: 0.7024 - val_loss: 0.1971 - val_accuracy: 0.6960\n","Epoch 206/500\n","200/200 - 5s - loss: 0.1870 - accuracy: 0.7065 - val_loss: 0.1972 - val_accuracy: 0.6888\n","Epoch 207/500\n","200/200 - 5s - loss: 0.1876 - accuracy: 0.7032 - val_loss: 0.1992 - val_accuracy: 0.6761\n","Epoch 208/500\n","200/200 - 5s - loss: 0.1865 - accuracy: 0.7079 - val_loss: 0.2013 - val_accuracy: 0.6793\n","Epoch 209/500\n","200/200 - 5s - loss: 0.1880 - accuracy: 0.7060 - val_loss: 0.1967 - val_accuracy: 0.6922\n","Epoch 210/500\n","200/200 - 5s - loss: 0.1904 - accuracy: 0.6983 - val_loss: 0.2018 - val_accuracy: 0.6812\n","Epoch 211/500\n","200/200 - 5s - loss: 0.1884 - accuracy: 0.6981 - val_loss: 0.1975 - val_accuracy: 0.6951\n","Epoch 212/500\n","200/200 - 5s - loss: 0.1856 - accuracy: 0.7054 - val_loss: 0.2128 - val_accuracy: 0.6392\n","Epoch 213/500\n","200/200 - 5s - loss: 0.1875 - accuracy: 0.7041 - val_loss: 0.2040 - val_accuracy: 0.6777\n","Epoch 214/500\n","200/200 - 5s - loss: 0.1887 - accuracy: 0.7036 - val_loss: 0.2091 - val_accuracy: 0.6676\n","Epoch 215/500\n","200/200 - 5s - loss: 0.1868 - accuracy: 0.7088 - val_loss: 0.1953 - val_accuracy: 0.7004\n","Epoch 216/500\n","200/200 - 5s - loss: 0.1850 - accuracy: 0.7123 - val_loss: 0.1954 - val_accuracy: 0.6948\n","Epoch 217/500\n","200/200 - 5s - loss: 0.1867 - accuracy: 0.7106 - val_loss: 0.1961 - val_accuracy: 0.6900\n","Epoch 218/500\n","200/200 - 5s - loss: 0.1844 - accuracy: 0.7129 - val_loss: 0.2059 - val_accuracy: 0.6824\n","Epoch 219/500\n","200/200 - 5s - loss: 0.1845 - accuracy: 0.7037 - val_loss: 0.1967 - val_accuracy: 0.6948\n","Epoch 220/500\n","200/200 - 5s - loss: 0.1871 - accuracy: 0.7093 - val_loss: 0.1973 - val_accuracy: 0.6891\n","Epoch 221/500\n","200/200 - 5s - loss: 0.1887 - accuracy: 0.7076 - val_loss: 0.1958 - val_accuracy: 0.6938\n","Epoch 222/500\n","200/200 - 5s - loss: 0.1868 - accuracy: 0.7069 - val_loss: 0.1935 - val_accuracy: 0.7027\n","Epoch 223/500\n","200/200 - 5s - loss: 0.1846 - accuracy: 0.7149 - val_loss: 0.1928 - val_accuracy: 0.7061\n","Epoch 224/500\n","200/200 - 5s - loss: 0.1859 - accuracy: 0.7097 - val_loss: 0.1981 - val_accuracy: 0.6783\n","Epoch 225/500\n","200/200 - 5s - loss: 0.1850 - accuracy: 0.7115 - val_loss: 0.1945 - val_accuracy: 0.7014\n","Epoch 226/500\n","200/200 - 5s - loss: 0.1873 - accuracy: 0.7050 - val_loss: 0.1951 - val_accuracy: 0.6979\n","Epoch 227/500\n","200/200 - 5s - loss: 0.1870 - accuracy: 0.7036 - val_loss: 0.1936 - val_accuracy: 0.6910\n","Epoch 228/500\n","200/200 - 5s - loss: 0.1841 - accuracy: 0.7132 - val_loss: 0.1941 - val_accuracy: 0.7004\n","Epoch 229/500\n","200/200 - 5s - loss: 0.1845 - accuracy: 0.7106 - val_loss: 0.1941 - val_accuracy: 0.7020\n","Epoch 230/500\n","200/200 - 5s - loss: 0.1885 - accuracy: 0.7048 - val_loss: 0.2046 - val_accuracy: 0.6862\n","Epoch 231/500\n","200/200 - 5s - loss: 0.1907 - accuracy: 0.6963 - val_loss: 0.1949 - val_accuracy: 0.6957\n","Epoch 232/500\n","200/200 - 5s - loss: 0.1863 - accuracy: 0.7133 - val_loss: 0.2001 - val_accuracy: 0.6881\n","Epoch 233/500\n","200/200 - 5s - loss: 0.1869 - accuracy: 0.7111 - val_loss: 0.1969 - val_accuracy: 0.6910\n","Epoch 234/500\n","200/200 - 5s - loss: 0.1830 - accuracy: 0.7144 - val_loss: 0.1950 - val_accuracy: 0.6922\n","Epoch 235/500\n","200/200 - 5s - loss: 0.1887 - accuracy: 0.7042 - val_loss: 0.2074 - val_accuracy: 0.6730\n","Epoch 236/500\n","200/200 - 5s - loss: 0.1847 - accuracy: 0.7121 - val_loss: 0.1932 - val_accuracy: 0.7023\n","Epoch 237/500\n","200/200 - 5s - loss: 0.1840 - accuracy: 0.7192 - val_loss: 0.1929 - val_accuracy: 0.6989\n","Epoch 238/500\n","200/200 - 5s - loss: 0.1855 - accuracy: 0.7095 - val_loss: 0.1933 - val_accuracy: 0.7017\n","Epoch 239/500\n","200/200 - 5s - loss: 0.1846 - accuracy: 0.7143 - val_loss: 0.1987 - val_accuracy: 0.6878\n","Epoch 240/500\n","200/200 - 5s - loss: 0.1833 - accuracy: 0.7212 - val_loss: 0.1949 - val_accuracy: 0.7011\n","Epoch 241/500\n","200/200 - 5s - loss: 0.1839 - accuracy: 0.7149 - val_loss: 0.1926 - val_accuracy: 0.7020\n","Epoch 242/500\n","200/200 - 5s - loss: 0.1835 - accuracy: 0.7161 - val_loss: 0.1920 - val_accuracy: 0.7042\n","Epoch 243/500\n","200/200 - 5s - loss: 0.1862 - accuracy: 0.7076 - val_loss: 0.1921 - val_accuracy: 0.7017\n","Epoch 244/500\n","200/200 - 5s - loss: 0.1839 - accuracy: 0.7199 - val_loss: 0.1944 - val_accuracy: 0.6944\n","Epoch 245/500\n","200/200 - 5s - loss: 0.1822 - accuracy: 0.7195 - val_loss: 0.1936 - val_accuracy: 0.6985\n","Epoch 246/500\n","200/200 - 5s - loss: 0.1841 - accuracy: 0.7189 - val_loss: 0.2018 - val_accuracy: 0.6730\n","Epoch 247/500\n","200/200 - 5s - loss: 0.1820 - accuracy: 0.7210 - val_loss: 0.1926 - val_accuracy: 0.7061\n","Epoch 248/500\n","200/200 - 5s - loss: 0.1835 - accuracy: 0.7173 - val_loss: 0.1978 - val_accuracy: 0.6900\n","Epoch 249/500\n","200/200 - 5s - loss: 0.1843 - accuracy: 0.7133 - val_loss: 0.2001 - val_accuracy: 0.6834\n","Epoch 250/500\n","200/200 - 5s - loss: 0.1835 - accuracy: 0.7187 - val_loss: 0.1973 - val_accuracy: 0.6951\n","Epoch 251/500\n","200/200 - 5s - loss: 0.1836 - accuracy: 0.7144 - val_loss: 0.1980 - val_accuracy: 0.6783\n","Epoch 252/500\n","200/200 - 5s - loss: 0.1818 - accuracy: 0.7225 - val_loss: 0.2101 - val_accuracy: 0.6635\n","Epoch 253/500\n","200/200 - 5s - loss: 0.1836 - accuracy: 0.7217 - val_loss: 0.1906 - val_accuracy: 0.7052\n","Epoch 254/500\n","200/200 - 5s - loss: 0.1824 - accuracy: 0.7241 - val_loss: 0.1939 - val_accuracy: 0.6963\n","Epoch 255/500\n","200/200 - 5s - loss: 0.1820 - accuracy: 0.7217 - val_loss: 0.1915 - val_accuracy: 0.7099\n","Epoch 256/500\n","200/200 - 5s - loss: 0.1845 - accuracy: 0.7171 - val_loss: 0.1952 - val_accuracy: 0.6976\n","Epoch 257/500\n","200/200 - 5s - loss: 0.1834 - accuracy: 0.7202 - val_loss: 0.1936 - val_accuracy: 0.6979\n","Epoch 258/500\n","200/200 - 5s - loss: 0.1833 - accuracy: 0.7157 - val_loss: 0.1927 - val_accuracy: 0.7004\n","Epoch 259/500\n","200/200 - 5s - loss: 0.1819 - accuracy: 0.7243 - val_loss: 0.1974 - val_accuracy: 0.6951\n","Epoch 260/500\n","200/200 - 5s - loss: 0.1810 - accuracy: 0.7280 - val_loss: 0.1912 - val_accuracy: 0.7042\n","Epoch 261/500\n","200/200 - 5s - loss: 0.1819 - accuracy: 0.7205 - val_loss: 0.1923 - val_accuracy: 0.7061\n","Epoch 262/500\n","200/200 - 5s - loss: 0.1836 - accuracy: 0.7176 - val_loss: 0.1911 - val_accuracy: 0.7058\n","Epoch 263/500\n","200/200 - 5s - loss: 0.1817 - accuracy: 0.7250 - val_loss: 0.1918 - val_accuracy: 0.7086\n","Epoch 264/500\n","200/200 - 5s - loss: 0.1829 - accuracy: 0.7197 - val_loss: 0.2016 - val_accuracy: 0.6869\n","Epoch 265/500\n","200/200 - 5s - loss: 0.1822 - accuracy: 0.7238 - val_loss: 0.1959 - val_accuracy: 0.6982\n","Epoch 266/500\n","200/200 - 5s - loss: 0.1809 - accuracy: 0.7230 - val_loss: 0.1935 - val_accuracy: 0.7058\n","Epoch 267/500\n","200/200 - 5s - loss: 0.1788 - accuracy: 0.7274 - val_loss: 0.1917 - val_accuracy: 0.7105\n","Epoch 268/500\n","200/200 - 5s - loss: 0.1816 - accuracy: 0.7206 - val_loss: 0.1934 - val_accuracy: 0.7042\n","Epoch 269/500\n","200/200 - 5s - loss: 0.1839 - accuracy: 0.7185 - val_loss: 0.1957 - val_accuracy: 0.6979\n","Epoch 270/500\n","200/200 - 5s - loss: 0.1798 - accuracy: 0.7228 - val_loss: 0.1945 - val_accuracy: 0.7033\n","Epoch 271/500\n","200/200 - 5s - loss: 0.1810 - accuracy: 0.7267 - val_loss: 0.1903 - val_accuracy: 0.7086\n","Epoch 272/500\n","200/200 - 5s - loss: 0.1787 - accuracy: 0.7295 - val_loss: 0.1987 - val_accuracy: 0.6881\n","Epoch 273/500\n","200/200 - 5s - loss: 0.1810 - accuracy: 0.7257 - val_loss: 0.1932 - val_accuracy: 0.7014\n","Epoch 274/500\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-dc4eb3893647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m history = model.fit(np.array(data[0:10000]), np.array(target[0:10000]), epochs=epoch,\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     batch_size=batchS, verbose=2)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"BVJCgz3bA4A3","colab_type":"text"},"source":["## Test the model"]},{"cell_type":"markdown","metadata":{"id":"3MwCq0LpA59G","colab_type":"text"},"source":["We are going to test the model with 20 % of the total videos. This videos have not been used to train the network. "]},{"cell_type":"code","metadata":{"id":"VDXFFy6zBG3X","colab_type":"code","colab":{}},"source":["result = model.evaluate(np.array(data_test), np.array(target_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YG-bPW4BL6U","colab_type":"text"},"source":["## Print the model accuracy"]},{"cell_type":"code","metadata":{"id":"wBV2t2Q6BOt9","colab_type":"code","colab":{}},"source":["for name, value in zip(model.metrics_names, result):\n","    print(name, value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xo-h0kPUr1ze","colab_type":"text"},"source":["acc, val_acc => accuracy, val_accuracy로 수정을 요함"]},{"cell_type":"code","metadata":{"id":"zO6YSbxgBZ3f","colab_type":"code","colab":{}},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path.eps', format='eps', dpi=1000)\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path1.eps', format='eps', dpi=1000)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCD5KrYwsKA-","colab_type":"text"},"source":["우리 프로젝트에 활용하려면?\n","\n","- 훈련된 모델을 h5파일로 전환\n","\n","- 실시간 영상 데이터를 20 프레임 단위로 쪼개어(프레임 resizing도 필요)\n","\n","  VGG16층의 15층까지의 층을 거쳐 (20, 4096)크기의 전이 값 텐서로 전환해야 \n","  \n","  우리가 만든 모델에 제공돼서 predict할 수 있다.\n","\n","  (실시간으로 할 수 있을까? 할 수 없다면 속도(반응)은 느리겠지만 어떻게 해야 \n","  \n","  하는가?)\n","\n","- 어떤 프로그램 로직을 통해 챗봇 알림을 보낼 것인가?\n","\n","- 영상화면에 폭력이 감지되었다는 것을 어떻게 비주얼 적으로 표현해 주는 것\n","\n","  주의사항\n","\n","- 그래프를 통해 알 수 있는 것은 epoch 23쯤부터 과적합이 발생한다는 것->최적화 \n","\n","  시키자.=>(코드 오류를 수정하니 과적합이 발생하지 않았다)\n","\n","- 프로젝트 발표 때 마지막 LSTM단의 사용 이유에 대해 정확히(애매모호하지 않게) \n","\n","  말할 수 있도록 제반 지식이 있다면 좋을 것 같다.\n","\n","- 데이터만 바뀌면 다른 주제로 얼마든지 전향 가능하지 않을까? (ex 충돌 감지)"]},{"cell_type":"markdown","metadata":{"id":"gXzWuILfzspv","colab_type":"text"},"source":["### 모델 저장 및 weights 저장"]},{"cell_type":"code","metadata":{"id":"hMiqwKIjgTcg","colab_type":"code","colab":{}},"source":["model_json = model.to_json()\n","# Save model with json format\n","with open(\"violence_detection_model.json\",\"w\") as json_file:\n","    json_file.write(model_json)\n","\n","# Save weight with h5 format\n","model.save_weights(\"violence_detection_model.h5\")"],"execution_count":null,"outputs":[]}]}