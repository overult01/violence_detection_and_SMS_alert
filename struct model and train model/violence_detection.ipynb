{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"violence_detection_final.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-x-x2hhlOKkl","colab_type":"text"},"source":["# Violence Detection using CNN + LSTM neural netowrk"]},{"cell_type":"markdown","metadata":{"id":"9fG4ju6NOSIv","colab_type":"text"},"source":["by [Pedro.F Rodenas (Github)](https://github.com/pedrofrodenas)"]},{"cell_type":"markdown","metadata":{"id":"nRQW1dLqOVO_","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"a1iagEQ3OXEn","colab_type":"text"},"source":["Today, the amount of public violence has increased dramatically. As much in high schools as in the street. This has resulted in the ubiquitous use of surveillance cameras. This has helped the authorities to identify these events and take the necessary measures. But almost all systems today require the human-inspection of these videos to identify such events, which is virtually inefficient. It is therefore necessary to have such a practical system that can automatically monitor and identify the surveillance videos.\n","The development of various deep learning techniques, thanks to the availability of large data sets and computational resources, has resulted in a historic change in the community of computer vision. Various techniques have been developed to address problems such as object detection, recognition, tracking, action recognition, legend generation, etc. However, despite recent developments in deep learning, very few techniques based on deep learning have been proposed to address the problem of detecting violence from videos."]},{"cell_type":"markdown","metadata":{"id":"L8w5vbqMObaw","colab_type":"text"},"source":["## Flowchart"]},{"cell_type":"markdown","metadata":{"id":"6kz7yE3kOgMk","colab_type":"text"},"source":["The method consists of extracting a set of frames belonging to the video, sending them to a pretrained network called VGG16, obtaining the output of one of its final layers and from these outputs train another network architecture with a type of special neurons called LSTM. These neurons have memory and are able to analyze the temporal information of the video, if at any time they detect violence, it will be classified as a violent video.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UVXa7uMnOlkp","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"oDfDnlliPMd-","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import cv2\n","import os\n","import numpy as np\n","import keras\n","import matplotlib.pyplot as plt\n","import download\n","from random import shuffle\n","from keras.applications import VGG16\n","from keras import backend as K\n","from keras.models import Model, Sequential\n","from keras.layers import Input\n","from keras.layers import LSTM\n","from keras.layers import Dense, Activation\n","import sys\n","import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFU6Qg8mPXvv","colab_type":"code","colab":{}},"source":["keras.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6pf1l28PzIO","colab_type":"text"},"source":["## Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"WKnyJkf8PzxE","colab_type":"text"},"source":["We will use the function ```print_progress``` to print the amount of videos processed and ```download_data``` to download the datasets"]},{"cell_type":"code","metadata":{"id":"qnafWmS7P3CG","colab_type":"code","colab":{}},"source":["def print_progress(count, max_count):\n","    # Percentage completion.\n","    pct_complete = count / max_count\n","\n","    # Status-message. Note the \\r which means the line should\n","    # overwrite itself.\n","    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n","\n","    # Print it.\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1V5cxltP5U-","colab_type":"code","colab":{}},"source":["#프로젝트 폴더에 data폴더를 생성하고 훈련 및 테스트를 위한 영상 데이터를 로드하는 함수\n","def download_data(in_dir, url):\n","    \n","    if not os.path.exists(in_dir):\n","        os.makedirs(in_dir)\n","    \n","    download.maybe_download_and_extract(url,in_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRf-KgkjP9Kt","colab_type":"text"},"source":["## Load Data\n","\n","Firstly, we define the directory to place the video dataset"]},{"cell_type":"code","metadata":{"id":"RiRKgwBgP-NY","colab_type":"code","colab":{}},"source":["in_dir = \"data\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reLkhZglQBrD","colab_type":"text"},"source":["We set the url to download the dataset"]},{"cell_type":"code","metadata":{"id":"YX0Qk202QCvw","colab_type":"code","colab":{}},"source":["url_hockey = \"http://visilab.etsii.uclm.es/personas/oscar/FightDetection/HockeyFights.zip\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7zrKRupRFaz","colab_type":"text"},"source":["to download the dataset and decompress it:"]},{"cell_type":"code","metadata":{"id":"urX5VyFzRF-o","colab_type":"code","colab":{}},"source":["#프로젝트의 data폴더에 훈련 및 테스트를 위한 영상 로드\n","download_data(in_dir,url_hockey)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCZHrpJjRKky","colab_type":"text"},"source":["Copy some of the data-dimensions for convenience."]},{"cell_type":"code","metadata":{"id":"SXTNEj6SRLZZ","colab_type":"code","colab":{}},"source":["# Frame size  \n","img_size = 224\n","\n","img_size_touple = (img_size, img_size)\n","\n","# Number of channels (RGB)\n","num_channels = 3\n","\n","# Flat frame size\n","img_size_flat = img_size * img_size * num_channels\n","\n","# Number of classes for classification (Violence-No Violence)\n","num_classes = 2\n","\n","# Number of files to train\n","_num_files_train = 1\n","\n","# Number of frames per video\n","_images_per_file = 20\n","\n","# Number of frames per training set\n","_num_images_train = _num_files_train * _images_per_file\n","\n","# Video extension\n","video_exts = \".avi\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wodq7EaSRSS8","colab_type":"text"},"source":["### Helper-function for getting video frames\n","Function used to get 20 frames from a video file and convert the frame to a suitable format for the neural net."]},{"cell_type":"code","metadata":{"id":"ciJEA5rRP9_g","colab_type":"code","colab":{}},"source":["#테스트를 위해 추가한 dummy code cell입니다.\n","#images 넘파이 배열의 shape -> 4차원 (frame 수, width, height, channel) -> (20, 224, 224, 3)\n","\n","images = []\n","for i in range(20):\n","    images.append(np.zeros((24, 24, 3)))\n","\n","images = np.array(images)\n","images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eu9c4a-3RVkO","colab_type":"code","colab":{}},"source":["#폴더와 파일명을 입력받아 동영상 파일을 이용해 20개의 image frame 생성하고 반환하는 함수\n","def get_frames(current_dir, file_name):\n","    \n","    in_file = os.path.join(current_dir, file_name)\n","    \n","    images = []\n","    \n","    vidcap = cv2.VideoCapture(in_file)\n","    \n","    success,image = vidcap.read()\n","        \n","    count = 0\n","\n","    #images_per_file == 20\n","    while count<_images_per_file:\n","        #BGR형식을 RGB로 형식으로 변경\n","        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        \n","        #이미지를 224 * 224 size로 변환하고 보간법 적용\n","        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n","                                 interpolation=cv2.INTER_CUBIC)\n","    \n","        images.append(res)\n","    \n","        success,image = vidcap.read()\n","    \n","        count += 1\n","        \n","    resul = np.array(images)\n","    #0~255사이의 정수 값을 신경망의 input으로 적합하게 0~1사이의 float 값으로 전처리\n","    resul = (resul / 255.).astype(np.float16)\n","    \n","    #resul의 자료형은 numpy.array, shape는 (20, 224, 224, 3)\n","    return resul"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLCjYFBtRZb-","colab_type":"text"},"source":["### Helper function to get the names of the data downloaded and label it"]},{"cell_type":"code","metadata":{"id":"79S0aZcQO8vf","colab_type":"code","colab":{}},"source":["# 테스트를 위해 추가한 dummy code cell입니다.\n","# zip과 shuffle역할 확인\n","\n","a = [1, 2, 3]\n","b = [4, 5, 6]\n","\n","t = list(zip(a,b))\n","\n","shuffle(t)\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEp1grRNUKzX","colab_type":"code","colab":{}},"source":["# 테스트를 위해 추가한 dummy code cell입니다.\n","# label_video_names의 return 값의 구조를 확인하기 위한 코드\n","\n","test_names = ['fi01.avi', 'fi02.avi','no01.avi','no02.avi']\n","test_labels = [[1,0], [1, 0], [0, 1], [0, 1]]\n","\n","test_c = list(zip(test_names, test_labels))\n","\n","shuffle(test_c)\n","\n","test_names, test_labels = zip(*test_c)\n","\n","test_names, test_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qiv5NIJjRbIA","colab_type":"code","colab":{}},"source":["# data폴더 안의 영상파일들의 이름('fi' 또는 'no'로 시작)을 통해 폭력영상인지 비폭력 영상인지 레이블링([1,0] , [0,1])하고\n","# 랜덤하게 섞은 뒤 영상파일 이름, 레이블로 구성된 튜플 반환\n","def label_video_names(in_dir):\n","    \n","    # list containing video names\n","    names = []\n","    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n","    labels = []\n","    \n","    \n","    for current_dir, dir_names,file_names in os.walk(in_dir):\n","        \n","        for file_name in file_names:\n","            \n","            if file_name[0:2] == 'fi':\n","                labels.append([1,0])\n","                names.append(file_name)\n","            elif file_name[0:2] == 'no':\n","                labels.append([0,1])\n","                names.append(file_name)\n","                     \n","            \n","    c = list(zip(names,labels))\n","    # Suffle the data (names and labels)\n","    shuffle(c)\n","    \n","    names, labels = zip(*c)\n","            \n","    return names, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3KW2kfgReKn","colab_type":"text"},"source":["### Plot a video frame to see if data is correct"]},{"cell_type":"code","metadata":{"id":"dIsaAgcyRfIx","colab_type":"code","colab":{}},"source":["# First get the names and labels of the whole videos\n","names, labels = label_video_names(in_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFMDv12XWbNS","colab_type":"code","colab":{}},"source":["# names, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqucOMsJRgqm","colab_type":"text"},"source":["Then we are going to load 20 frames of one video, for example"]},{"cell_type":"code","metadata":{"id":"xUfZO-0BRj0f","colab_type":"code","colab":{}},"source":["names[12]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORGJ2pS9RnWw","colab_type":"text"},"source":["The video has violence, look at the name of the video, starts with 'fi'"]},{"cell_type":"code","metadata":{"id":"EqBi8z6rRoMW","colab_type":"code","colab":{}},"source":["frames = get_frames(in_dir, names[12])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UA6CQgLoRNVl","colab_type":"code","colab":{}},"source":["frames.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtgQEmI6RrmM","colab_type":"text"},"source":["Convert back the frames to uint8 pixel format to plot the frame"]},{"cell_type":"code","metadata":{"id":"9ihSA_ogRsNU","colab_type":"code","colab":{}},"source":["#이미지를 확인하기 위해 전처리 전으로 재변환\n","visible_frame = (frames*255).astype('uint8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM1kNhaHRvSv","colab_type":"code","colab":{}},"source":["plt.imshow(visible_frame[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_gVWtYPvR8n2","colab_type":"code","colab":{}},"source":["plt.imshow(visible_frame[15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sF1QieG5SANp","colab_type":"text"},"source":["## Pre-Trained Model: VGG16"]},{"cell_type":"markdown","metadata":{"id":"0o6FA6slSA1l","colab_type":"text"},"source":["The following creates an instance of the pre-trained VGG16 model using the Keras API. This automatically downloads the required files if you don't have them already.\n","\n","The VGG16 model contains a convolutional part and a fully-connected (or dense) part which is used for classification. If include_top=True then the whole VGG16 model is downloaded which is about 528 MB. If include_top=False then only the convolutional part of the VGG16 model is downloaded which is just 57 MB."]},{"cell_type":"code","metadata":{"id":"MjRN6oE4SC81","colab_type":"code","colab":{}},"source":["#분류기층과 imagenet을 통해 학습한 가중치를 포함 포함한 VGG16층 로드, \n","image_model = VGG16(include_top=True, weights='imagenet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_7z-y1mSPov","colab_type":"text"},"source":["Let's see the model summary"]},{"cell_type":"code","metadata":{"id":"ud7OU0t7SQPi","colab_type":"code","colab":{}},"source":["image_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-NxnGLlDSTwr","colab_type":"text"},"source":["We can observe the shape of the tensors expected as input by the pre-trained VGG16 model. In this case it is images of shape 224 x 224 x 3. Note that we have defined the frame size as 224x224x3. The video frame will be the input of the VGG16 net."]},{"cell_type":"code","metadata":{"id":"gtldUMidR4rz","colab_type":"code","colab":{}},"source":["#아래의 코드가 이렇게 수정되야 하지 않을까?\n","image_model.layers[0].output_shape[0][1:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a0k89M5SUjP","colab_type":"code","colab":{}},"source":["input_shape = image_model.layers[0].output_shape[1:3]\n","input_shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKYNH7V7RpON","colab_type":"code","colab":{}},"source":["#수정한 코드\n","input_shape = image_model.layers[0].output_shape[0][1:3]\n","input_shape\n","\n","#VGG16의 input size는 (batch크기, 224, 224, 3)이 될 것이다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMR6iqg4SaOl","colab_type":"text"},"source":["### VGG16 model flowchart"]},{"cell_type":"markdown","metadata":{"id":"qFUDqTiBSa0H","colab_type":"text"},"source":["The following chart shows how the data flows when using the VGG16 model for Transfer Learning. First we input and process 20 video frames in batch with the VGG16 model. Just prior to the final classification layer of the VGG16 model, we save the so-called Transfer Values to a cache-file.\n","\n","The reason for using a cache-file is that it takes a long time to process an image with the VGG16 model. If each image is processed more than once then we can save a lot of time by caching the transfer-values.\n","\n","When all the videos have been processed through the VGG16 model and the resulting transfer-values saved to a cache file, then we can use those transfer-values as the input to LSTM neural network. We will then train the second neural network using the classes from the violence dataset (Violence, No-Violence), so the network learns how to classify images based on the transfer-values from the VGG16 model."]},{"cell_type":"markdown","metadata":{"id":"5ZAgDy11SuDH","colab_type":"text"},"source":["다음의 차트는 전이학습을 위해 VGG16모델을 사용할 때 어떻게 data가 흐르는지 보여줍니다. 먼저 우리는 20개의 비디오 프레임들을 VGG16모델에 배치로 넣고 처리합니다. 그리고 VGG16 모델의 최종 분류 레이어 직전에 소위 전송 값을 캐시 파일에 저장합니다. \n","\n","캐시파일을 사용하는 이유는 VGG16모델을 이용한 처리에 많은 시간이 걸리기 때문이며, 만약 이미지가 한번 이상 처리된다면 전이 값들을 캐싱하므로써 많은 시간을 절약할 수 있습니다.\n","\n","모든 비디오가 VGG16모델을 통해 처리되고 캐시된 전이 값을 가진다면, 우리는 그 전이 값을 LSTM신경망의 input으로 사용할 수 있다. 우리는 두번 째 신경망에서 폭력 데이터 세트(폭력, 비폭력)를 사용하여 훈련시킬 것이며 \n","따라서 신경망은 VGG16모델로부터의 전이값에 기반하여 이미지를 분류할지 배운다."]},{"cell_type":"code","metadata":{"id":"4YWFA-2tSdfB","colab_type":"code","colab":{}},"source":["# We will use the output of the layer prior to the final\n","# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n","transfer_layer = image_model.get_layer('fc2')\n","# keras 함수형 API를 이용\n","# vgg16의 마지막 층, 즉 최종 분류 층을 짜른 모형 생성.마지막 층을 없애고 15층까지 통과한 output을 LSTM신경망에 공급할 것이다.\n","image_model_transfer = Model(inputs=image_model.input,\n","                             outputs=transfer_layer.output)\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","\n","\n","print(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n","print(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL8B1zXHXa7q","colab_type":"code","colab":{}},"source":["#확인\n","print(K.int_shape(image_model_transfer.output)[1])\n","print(K.int_shape(transfer_layer.output)[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfU9vd8RbPCH","colab_type":"code","colab":{}},"source":["#확인\n","image_model_transfer.summary()\n","\n","#확인결과 VGG16 -> VGG15로 변환된 것을 알 수 있다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ghum6s2Si1n","colab_type":"text"},"source":["### Function to process 20 video frames through VGG16 and get transfer values"]},{"cell_type":"code","metadata":{"id":"-AdxYAtiSlLF","colab_type":"code","colab":{}},"source":["#비디오의 20frames를 vgg16모델 15층(fc2)까지로 구성된 모델을 거친  output tensor를 반환하는 함수 \n","def get_transfer_values(current_dir, file_name):\n","    \n","    # Pre-allocate input-batch-array for images.\n","    # shape == (20, 224, 224, 3)\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    # image_batch의 shape == (20, 224, 224, 3)\n","    image_batch = np.zeros(shape=shape, dtype=np.float16)\n","    image_batch = get_frames(current_dir, file_name)\n","      \n","    # Pre-allocate output-array for transfer-values.\n","    # Note that we use 16-bit floating-points to save memory.\n","\n","    # shape는 (20, 4096)\n","    shape = (_images_per_file, transfer_values_size)\n","    # transfer_values의 shape == (20, 4096)\n","    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","\n","    transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","        \n","    return transfer_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGdRIG6oSooG","colab_type":"text"},"source":["### Generator that process one video through VGG16 each function call"]},{"cell_type":"markdown","metadata":{"id":"r3MDDReeosF9","colab_type":"text"},"source":["아래의 proces_transfer()함수를 이해하기 위해서는 파이썬 Generator공부가 필요 https://wikidocs.net/16069"]},{"cell_type":"code","metadata":{"id":"9Rj6k7Ikvfd6","colab_type":"code","colab":{}},"source":["#Generator 확인을 위한 dummy code cell입니다.\n","#Generator 예제\n","def test_generator():\n","\n","    count = 0\n","\n","    while(count < 3):\n","        virtual_transfer_values = (np.random.random((20,4096)) * 10).astype('int32')\n","        virtual_labelss = (np.random.random((20,2)) * 10).astype('int32')\n","        yield virtual_transfer_values, virtual_labelss\n","    \n","\n","gen = test_generator()\n","\n","print(next(gen)[0].shape)\n","print(next(gen)[1].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3oRbPGZvvAp","colab_type":"code","colab":{}},"source":["#aux의 기능 확인\n","print(np.ones([20,2]) * [1, 0])\n","print(np.ones([20,2]) * [0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2BvaY3eSpSH","colab_type":"code","colab":{}},"source":["# 영상폴더 안의 영상 파일들을 모두 레이블링 값과 VGG16모델의 15층까지의 모델의 전이 값의 튜플로 반환하는 제너레이터 함수.\n","# 반복자를 생성 후 한번 호출할 때 마다 20 frame의 전이 값(20, 4096)과 레이블링 값(20, 2)를 반환\n","def proces_transfer(vid_names, in_dir, labels):\n","    \n","    count = 0\n","    #tam  = 영상 수\n","    tam = len(vid_names)\n","    \n","    # Pre-allocate input-batch-array for images.\n","    # shape(20, 224, 224, 3)\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    while count<tam:\n","        \n","        video_name = vid_names[count]\n","        # image_batch size (20, 224, 224, 3)\n","        image_batch = np.zeros(shape=shape, dtype=np.float16)\n","\n","        image_batch = get_frames(in_dir, video_name)\n","        \n","         # Note that we use 16-bit floating-points to save memory.\n","         # (20, 4096)\n","        shape = (_images_per_file, transfer_values_size)\n","        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","        \n","        # transfer_values 의 shape는 (20, 4096)\n","        transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","\n","        #labels[count] 는 이 영상이 폭력 영상인지 아닌지 [1, 0] 또는 [0, 1]의 값을 가진다.\n","        labels1 = labels[count]\n","        \n","        #이렇게 하는 이유는 폭력 영상이라면 영상을 구성하는 20 frame이 모두 폭력 frame이기 때문에 모든 frame에 labeling을 하는 것이다.\n","        aux = np.ones([20,2])\n","        \n","        labelss = labels1*aux\n","        # Generator 생성 키워드\n","        # ((20,4096), (20, 2))\n","        yield transfer_values, labelss\n","        \n","        count+=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-Cnv0fRSswF","colab_type":"text"},"source":["### Functions to save transfer values from VGG16 to later use\n","We are going to define functions to get the transfer values from VGG16 with defined number of files. Then save the transfer values files used from training in one file and the ones uses for testing in another one. "]},{"cell_type":"markdown","metadata":{"id":"RD9UDaVVawoL","colab_type":"text"},"source":["개조된 VGG16, 즉  VGG15를 통과한 전이 값은 \n","\n","학습이 반복되도 더 이상 학습되지 않는 값이므로 \n","\n","파일화하여 재계산 없이 빠른 접근이 가능하도록 h5파일로 만든다\n","\n","훈련용 파일과 테스트용 파일을 분리해서 생성"]},{"cell_type":"code","metadata":{"id":"tvU53ypSSvL0","colab_type":"code","colab":{}},"source":["def make_files(n_files):\n","    \n","    gen = proces_transfer(names_training, in_dir, labels_training)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0] #20\n","    row_count2 = chunk[1].shape[0] #20\n","    \n","    with h5py.File('prueba.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:] #(None, 4096)\n","        maxshape2 = (None,) + chunk[1].shape[1:] #(None, 2)\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            dset2[row_count:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nK8uFExS0nX","colab_type":"code","colab":{}},"source":["def make_files_test(n_files):\n","    \n","    gen = proces_transfer(names_test, in_dir, labels_test)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0]\n","    row_count2 = chunk[1].shape[0]\n","    \n","    with h5py.File('pruebavalidation.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:]\n","        maxshape2 = (None,) + chunk[1].shape[1:]\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            # 로직 오류는 발생하지 않을 수 있으나 row_count가 row_count2가 되는게 의미상 옳다.\n","            dset2[row_count2:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9axnZ8dS64T","colab_type":"text"},"source":["### Split the dataset into training set and test set\n","We are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy."]},{"cell_type":"code","metadata":{"id":"wjne3svdS9Y3","colab_type":"code","colab":{}},"source":["# sample수가 아주 많지 않기 때문에 셔플 시 폭력 영상과 비폭력 영상이 골고루 섞이지 않았다면\n","# 제대로 훈련이 되지 않을 수 있음 \n","# 문제해결 방법 모색 필요\n","training_set = int(len(names)*0.8)\n","test_set = int(len(names)*0.2)\n","\n","names_training = names[0:training_set]\n","names_test = names[training_set:]\n","\n","labels_training = labels[0:training_set]\n","labels_test = labels[training_set:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmiJi6G7TBnI","colab_type":"text"},"source":["Then we are going to process all video frames through VGG16 and save the transfer values."]},{"cell_type":"code","metadata":{"id":"pyO9WP-6TER4","colab_type":"code","colab":{}},"source":["make_files(training_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ThoefXUXPrS","colab_type":"code","colab":{}},"source":["make_files_test(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s2imcRixMAr","colab_type":"text"},"source":["### Load the cached transfer values into memory\n","We have already saved all the videos transfer values into disk. But we have to load those transfer values into memory in order to train the LSTM net. One question would be: why not process transfer values and load them into RAM memory? Yes is a more eficient way to train the second net. But if you have to train the LSTM in different ways in order to see which way gets the best accuracy, if you didn't save the transfer values into disk you would have to process the whole videos each training. It's very time consuming processing the videos through VGG16 net. \n"]},{"cell_type":"markdown","metadata":{"id":"RnK1JL-izzeS","colab_type":"text"},"source":["In order to load the saved transfer values into RAM memory we are going to use this two functions:"]},{"cell_type":"code","metadata":{"id":"Ez0blP2z0CsF","colab_type":"code","colab":{}},"source":["def process_alldata_training():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('prueba.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dne2XaQ90MGk","colab_type":"code","colab":{}},"source":["def process_alldata_test():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('pruebavalidation.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phyhoYc67VW8","colab_type":"code","colab":{}},"source":["data, target = process_alldata_training()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iajF2SbEjH2O","colab_type":"code","colab":{}},"source":["#data, target 확인\n","print(len(data))\n","print(len(data[0]))\n","print(len(data[0][0]))\n","print(len(target))\n","print(len(target[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJpXHvEg7Xhc","colab_type":"code","colab":{}},"source":["data_test, target_test = process_alldata_test()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dbarv56DkgG0","colab_type":"code","colab":{}},"source":["#data_test, target_test 확인\n","print(len(data_test))\n","print(len(data_test[0]))\n","print(len(data_test[0][0]))\n","print(len(target_test))\n","print(len(target_test[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qjaawyqj0arq","colab_type":"text"},"source":["##Recurrent Neural Network"]},{"cell_type":"markdown","metadata":{"id":"dXFhxfzE0hGb","colab_type":"text"},"source":["The basic building block in a Recurrent Neural Network (RNN) is a Recurrent Unit (RU). There are many different variants of recurrent units such as the rather clunky LSTM (Long-Short-Term-Memory) and the somewhat simpler GRU (Gated Recurrent Unit) which we will use in this tutorial. Experiments in the literature suggest that the LSTM and GRU have roughly similar performance. Even simpler variants also exist and the literature suggests that they may perform even better than both LSTM and GRU, but they are not implemented in Keras which we will use in this tutorial.\n","\n","A recurrent neuron has an internal state that is being updated every time the unit receives a new input. This internal state serves as a kind of memory. However, it is not a traditional kind of computer memory which stores bits that are either on or off. Instead the recurrent unit stores floating-point values in its memory-state, which are read and written using matrix-operations so the operations are all differentiable. This means the memory-state can store arbitrary floating-point values (although typically limited between -1.0 and 1.0) and the network can be trained like a normal neural network using Gradient Descent.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BsIrL3ix4Edp","colab_type":"text"},"source":["### Define LSTM architecture"]},{"cell_type":"markdown","metadata":{"id":"qFiSjMaC4Q1c","colab_type":"text"},"source":["When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n"]},{"cell_type":"markdown","metadata":{"id":"HAgSUeVu58N_","colab_type":"text"},"source":["The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n"]},{"cell_type":"code","metadata":{"id":"XWABZ91b6f7l","colab_type":"code","colab":{}},"source":["chunk_size = 4096\n","n_chunks = 20\n","rnn_size = 512\n","\n","model = Sequential()\n","model.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\n","model.add(Dense(1024))\n","model.add(Activation('relu'))\n","model.add(Dense(50))\n","model.add(Activation('sigmoid'))\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n","model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVonOPYW7F_b","colab_type":"text"},"source":["## Model training\n"]},{"cell_type":"code","metadata":{"id":"iRZlW4ZV_ygS","colab_type":"code","colab":{}},"source":["epoch = 200\n","batchS = 500\n","\n","history = model.fit(np.array(data[0:750]), np.array(target[0:750]), epochs=epoch,\n","                    validation_data=(np.array(data[750:]), np.array(target[750:])), \n","                    batch_size=batchS, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVJCgz3bA4A3","colab_type":"text"},"source":["## Test the model"]},{"cell_type":"markdown","metadata":{"id":"3MwCq0LpA59G","colab_type":"text"},"source":["We are going to test the model with 20 % of the total videos. This videos have not been used to train the network. "]},{"cell_type":"code","metadata":{"id":"VDXFFy6zBG3X","colab_type":"code","colab":{}},"source":["result = model.evaluate(np.array(data_test), np.array(target_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YG-bPW4BL6U","colab_type":"text"},"source":["## Print the model accuracy"]},{"cell_type":"code","metadata":{"id":"wBV2t2Q6BOt9","colab_type":"code","colab":{}},"source":["for name, value in zip(model.metrics_names, result):\n","    print(name, value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xo-h0kPUr1ze","colab_type":"text"},"source":["acc, val_acc => accuracy, val_accuracy로 수정을 요함"]},{"cell_type":"code","metadata":{"id":"zO6YSbxgBZ3f","colab_type":"code","colab":{}},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path.eps', format='eps', dpi=1000)\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path1.eps', format='eps', dpi=1000)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCD5KrYwsKA-","colab_type":"text"},"source":["우리 프로젝트에 활용하려면?\n","\n","- 훈련된 모델을 h5파일로 전환\n","\n","- 실시간 영상 데이터를 20 프레임 단위로 쪼개어(프레임 resizing도 필요)\n","\n","  VGG16층의 15층까지의 층을 거쳐 (20, 4096)크기의 전이 값 텐서로 전환해야 \n","  \n","  우리가 만든 모델에 제공돼서 predict할 수 있다.\n","\n","  (실시간으로 할 수 있을까? 할 수 없다면 속도(반응)은 느리겠지만 어떻게 해야 \n","  \n","  하는가?)\n","\n","- 어떤 프로그램 로직을 통해 챗봇 알림을 보낼 것인가?\n","\n","- 영상화면에 폭력이 감지되었다는 것을 어떻게 비주얼 적으로 표현해 주는 것\n","\n","  주의사항\n","\n","- 그래프를 통해 알 수 있는 것은 epoch 23쯤부터 과적합이 발생한다는 것->최적화 \n","\n","  시키자.=>(코드 오류를 수정하니 과적합이 발생하지 않았다)\n","\n","- 프로젝트 발표 때 마지막 LSTM단의 사용 이유에 대해 정확히(애매모호하지 않게) \n","\n","  말할 수 있도록 제반 지식이 있다면 좋을 것 같다.\n","\n","- 데이터만 바뀌면 다른 주제로 얼마든지 전향 가능하지 않을까? (ex 충돌 감지)"]},{"cell_type":"markdown","metadata":{"id":"gXzWuILfzspv","colab_type":"text"},"source":["### 모델 저장 및 weights 저장"]},{"cell_type":"code","metadata":{"id":"hMiqwKIjgTcg","colab_type":"code","colab":{}},"source":["model_json = model.to_json()\n","# Save model with json format\n","with open(\"violence_detection_model.json\",\"w\") as json_file:\n","    json_file.write(model_json)\n","\n","# Save weight with h5 format\n","model.save_weights(\"violence_detection_model.h5\")"],"execution_count":null,"outputs":[]}]}